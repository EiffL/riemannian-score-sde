\part{Ideas score manifold, things to try, equivariance, etc.}

\section{Score-based generative modelling}

The overall idea of score-based generative models is to construct a diffusion process mapping the data distribution $p_0$ into a tractable noise distribution---invariant wrt the diffusion process $p_T$.
The Wiener or Ornstein–Uhlenbeck processes are typical choices for the diffusion process.
It is known that a diffusion process is a Markov process under time reversal \cite{u.g.haussmann1986Time}.
Then one can generate new samples from the data distribution by sampling noise $x(T) \sim p_T$ and simulating the reverse diffusion process.
Below we briefly remind key steps of score-based diffusion models.

\subsection{Forward process: perturbating data}

% \todo[inline]{existence}
A diffusion process $\{X(t)\}_{t\in[0,T]}$ can be modelled as the solution to an Ito SDE [REF?]:
\begin{definition}[Ito SDE]
    \label{def:ito-sde}
    \begin{align}
    dX_t = f(X_t, t) dt + G(X_t, t) dB_t
    \end{align}
    with $B_t$ is a standard Brownian motion, $f: \R^d \times \R \rightarrow \R^d$ a drift coefficient and $G:\R^d \R \rightarrow \R^{d \times d}$ a diffusion coefficient.
\end{definition}
SDEs of this form can be discretised and simulated via a Euler-Maruyama discretisation:
\begin{definition}[Ito SDE discretisation]
For the set of times $0=\tau_0 < \dots < \tau_N = T$, $\Delta t_i = \tau_{i+1} - \tau_{i}$, and $\Delta B_k \sim B_{\tau_{k+1}} - B_{\tau_k}$, an Ito SDE can be discretised as 
\begin{align}
    X_{k+1} = X_k + f(X_k, \tau_k) \Delta t_k + G(X_k, \tau_k) \Delta B_k
\end{align}
\end{definition}

\subsection{Reverse process: generating data}
The reverse-time process $\{ Y_t \}_{t\in [0,T]} = \{ X_{T-t} \}_{t\in [0,T]}$ is also a diffusion, and we have the following result
\begin{theorem}[Reverse-time Ito SDE \cite{anderson1982reverse}]
For a Ito SDE of the form of \cref{def:ito-sde}, with certain smoothness and growth properties which guarantee existence and uniqueness of a solution  \cite{kushner1974stochastic}, and the existence of the probability density $p(X_t,t)$ for $t_0 \leq t\leq T$ as a smooth and unique solution of its associated Kolmogorov equation. Also define an $\bar{W}_t$ by $\bar{W}_{0} = 0$ and 
\begin{align}
    d\bar{W}_t = dW_t + \nabla \cdot g(Y_t, t) + g(Y_t, t) \nabla \log p(Y_t, t)
\end{align}
then (skipping some) a reverse time model for $X_t$ is defined by 
\begin{align}
    dY_t = \bar{f}(Y_t, t) dt + G(Y_t, t) d\bar{W}_t
\end{align}
%where $$\bar{f}^i(X_t, t) = f^i(X_t, t) - \frac{1}{p(X_t, t)}\sum_{j, k} \frac{\partial}{\partial X_t^j}[p(X_t, t) g^{ik}(X_t, t)g^{jk}(X_t, t)]$$
where
\begin{align}
    \bar{f}(Y_t, t) &= f(Y_t, t) - \nabla \cdot[G(Y_t, t)G(Y_t, t)^\top] - G(Y_t,t)G(Y_t, t)^\top \nabla \log p(Y_t, t) \label{eqn:rev_time_drift} 
\end{align}
\end{theorem}


% %
% The reverse-time process $\{Y_t\}_{t \in [0,T]} = \{X_{T-t}\}_{t \in [0,T]}$ is also a diffusion and satisfies \cite{u.g.haussmann1986Time}

% \begin{align}
% dY_t = [-f(Y_t, T-t) - g(T-t)^2 \nabla \log p_{T-t}(Y_t)] dt + g(T-t) dW.
% \end{align}
% This process can be simulated as
% \begin{align}
% X_k = (1 - \beta) X_{k+1} + 2 \nabla\log p_{k+1}(X_{k+1}) + \sqrt{2\beta} Z_{k+1}, \quad Z_{k+1} \sim \N(0, 1).
% \end{align}
\subsection{Density estimation}
%
For all diffusion processes, there exists a corresponding deterministic process whose trajectories share the same marginal probability densities $\{p(X_t)\}_{t\in[0,T]}$.

This deterministic process satisﬁes an ODE \cite{song2020score}
\begin{align} \label{eq:ode}
dX_t = { f(X_t, t) - \frac{1}{2} \nabla \cdot[G(X_t, t)G(X_t, t)^\top] - \frac{1}{2} G(X_t,t)G(X_t, t)^\top \nabla \log p(X_t, t) } dt
\end{align}
which can be solved with classical numerical solvers (e.g.\ Runge-Kutta).


\subsection{A special case}

A particular case of interest comes when we set
\begin{align}
    f(X_t, t) &= -\beta(t) X_t \\
    G(X_t, t) &= \sqrt{2 \beta(t)} \\
    \beta(t) &: \R \to \R
\end{align}
I.e.
\begin{align}
dX_t = - \beta(t) X_t dt + \sqrt{2\beta(t)} dB_t
\end{align}
For this SDE we can compute the following limiting invariant distribution
\begin{lemma}[Limiting distribution]
For an Ito SDE of the form 
\begin{align}
dX_t = - \beta X_t dt + \sqrt{2\beta} dB_t
\end{align}
the limiting distribution
\begin{align}
    \lim_{t \to \infty} p({X}_t) = \c{N}(0, I)
\end{align}
\end{lemma}
The form of \cref{eqn:rev_time_drift} becomes
\begin{align}
    \bar{f}(Y_t, t) &= - \beta(t) X_t - 2\beta(t) \nabla \log p(Y_t, t)\\
    &= -\beta(t)(X_t + 2 \nabla \log p(Y_t, t)
\end{align}

This special case will be of interest for what follows.


%
\subsection{Training}
In theory, we should be able to simply reverse the diffusion process from noise distribution to data to get our generative model.

Unfortunately the \emph{Stein score} $\nabla \log p(X_t, t)$ is not available analytically, and therefore needs to be estimated, typically by a neural network, $s_\theta: \R^n \times [0,T] \rightarrow \R^n$ such that $s_\theta^\star(X_t, t) \approx \nabla_{X_t} \log p_t(X_t)$.
As Stein score satisfies
\begin{align}
\nabla{X_t} \log p_t(X_t) = \E_{p_{0|t}}\left[ \nabla_{X_t} \log p(X_t|X_0) \right]
\end{align}
the optimal parameter $\theta^\star$ is optimised for such that
%
\begin{align}
\theta^\star \in \argmin_\theta ~\E_t \left\{ \lambda(t) \E_{X(0), X(t)} \left[ \left| s_\theta(X(t),t) - \nabla_{X(t)} \log p(X(t)|X(0))\right|^2_2  \right] \right\}
\end{align}
with $\lambda: [0,T] \rightarrow \R^+$ (a tuneable hyperparameter), $t \sim U([0, T])$, $X(0) \sim p_0(X) \triangleq p_{\text{data}}(X)$ and $X(t) \sim p(X(t)|X(0))$ (which is normally distributed with closed-form mean and variance).

\section{Extension: Generative modelling for functions}

We now move to the setting where we care about modelling a stochastic process $\bf{f} \sim p_{data}(\bf{f})$ with $f: \R^n \rightarrow \R^d$.
Let's assume we have access to sets of  samples 
\begin{align}
    { \{ \{x_1^j, \bf{y}_1^j\}, \dots,  \{x_{N_j}^j, \bf{y}_{N_j}^j}\} \}_{j=1}^N    
\end{align}
with $\bf{y}^j \sim f^j(x^j) + \bf{\epsilon}$ for several functions $\bf{f}^j \sim p_{data}(\bf{f})$.

We want to construct a diffusion process $\{\bf{f}_t\}_{t=0}^T$ on the space of stochastic processes such that $p_0=p_{data}$ and $p_T \approx p_{prior} = p_{\text{GP}}(0, \kappa)$ (wlog the mean function is assume to be 0). One could chose a white noise kernel for this GP prior, similar to the previous section, or a more structured kernel, which may specify a prior distribution closer to the data distribution of interest, and build in some prior knowledge about the data distribution.
As $\{\bf{f}_t\}_{t=0}^T$ is infinite dimensional, and in practice we only have access to finite data, we aim to construct a diffusion process on the \emph{finite-dimensional marginals} of the stochastic process $d\mY_t|\mX_{t=0}^T$ with $\mY_t = (\bf{y}_t^1, \dots, \bf{y}_t^M)$ conditioned on queries $\mX = (x^1, \dots, x^M)$. % so that $p_T(\mY_T | \x) \approx \N(\mY_T; 0,\kappa(\x)).$

\subsection{Forward process: perturbing data}
%
Let's construct the following Ito SDE
\begin{definition}[Finite-dimensional marginal Ito SDE]
\begin{align}
d\mY_t|\mX = -\beta \mY_t dt + \sqrt{2\beta} \kappa(\mX)^{1/2} ~d\mB_t
\end{align}
\end{definition}
We can compute the invariant distribution of this diffusion
\begin{align}
    \lim_{t \to \infty} p(\mY_t | \mX_t) = \c{N}(0, \kappa(\mX))
\end{align}
Note this is not white noise, but a structured distribution specified by the kernel.

% This diffusion can be discretised as
% \begin{align}
% p(\mY_{k+1}|\mY_{k},\mX) = \c{N}(\mY_{k+1}; - \beta \mY_{k} , 2\beta \kappa(\mX))
% \end{align}
% or equivalently
% \begin{align}
% \mY_{k+1} = (1 - \beta) \mY_{k} + \sqrt{2\beta} \kappa(\mX)^{1/2} \mz_{k}, \quad \text{with} \ \mz_{k} \sim \N(0, I_M).
% \end{align}
% \begin{align}
% p(\mY_{k+1}|\mY_{k},\mX) = \N(\mY_{k+1}; - \beta \mY_{k} ,\sqrt{2} \kappa(\mX))
% \end{align}

\subsection{Reverse process: generating data}
The reverse process is given by \cite{anderson1982reverse}.
\begin{align}
d\mY_{T-t}|\mX = \left[ -\beta \mY_{T-t} + 2 \kappa(\mX) \nabla_{\mY_{T-t}} \log p_{T-t}(\mY_{T-t} | \mX)\right] dt + \sqrt{2\beta} \kappa(\mX)^{1/2} ~d\mB_t
\end{align}
%
% which can be discretised as
% \begin{align}
% p(\mY_{k}|\mY_{k+1},\x) = \c{N}(\mY_{k+1}; - \beta \mY_{k+1} + 2 \kappa(\mX) \nabla_{\mY_{k+1}} \log p(\mY_{k+1} | \mX), 2\beta \kappa(\mX))
% \end{align}
% or equivalently
% \begin{align}
% \mY_{k} = (1 - \beta) \mY_{k+1} + 2 \kappa(\mX) \nabla_{\mY_{k+1}} \log p(\mY_{k+1} | \mX) + \sqrt{2\beta} \kappa(\mX)^{1/2} \mz_{k+1}.
% \end{align}

\paragraph{Sampling}
So as to sample $f_0 \sim p_0$ evaluated at $\mY_0=f_0(x)$, we first sample $f_T \sim p_{\text{GP}}(0, \kappa)$, evaluate $\mY_T = f(x)$, and eventually diffuse $\mY|x$ as described above.

\subsection{Training}

\paragraph{Score network}
The Stein score is estimated by a neural network $s_\theta: (\R^n \times \R)^M \times [0,T] \rightarrow \R^M$ such that $s_\theta^\star(\mY_t, x, t) \approx \nabla_{\mY_t} \log p_t(\mY_t|x)$.
The ordering of the samples is arbitrary so $s_\theta$ should be permutation invariant so that $s_\theta^\star(\sigma(\mY_t), \sigma(x), t) = s_\theta^\star(\mY_t, x, t)$ for any permutation $\sigma \in \Sigma_M$.
Self-attention should also help.
The architecture should be inspired from the ones used in Neural Processes \cite{garnelo2018conditional,garnelo2018neural}.

\todo[inline]{Constraints from consistency?}

\paragraph{Loss}
The score network parameters $\theta$ are optimised such that
\begin{align}
\theta^\star \in \argmin_\theta ~\E_t \left\{ \lambda(t) \E_{\mY(0),\mY(t)|\mX} \left[ \left| s_\theta(\mY(t),\mX,t) - \nabla_{\mY(t)|\mX} \log p(\mY(t)|\mY(0),\mX) \right|^2_2  \right] \right\}
\end{align}
with $\lambda: [0,T] \rightarrow \R^+$, $t \sim U([0, T])$.

\subsection{Stationary}

One typically common equivairance in SPs is stationarity, it might be good to see how one would build this in give some data. We can also extend to more general symmetries al la \cite{holderrieth2021equivariant}. 

Thinking in euclidean terms:
\begin{itemize}
    \item Translation equivariace can possibly come from a similar method to the point clopud paper with Marcel
    \item Rotations etc from constraining the score function
\end{itemize}

\subsection{Conditioning on observations}

Consider that we have observed some already observed data points $\{(\mathbf{x}_i, y_i)\}_{i=1}^{N_i}$ and we want to evaluated the SP on the
SP on some new points $\{ \mathbf{x}_i \}_{i=1}^{\bar{N}_i}$. Can we construct such a process?

Two directions maybe
\begin{itemize}
    \item Can we train the SP only on observed samples, then condition the "noise" GP on the observations, sample from this, then evolve those to get conditioned samples?
    \item We could train the score functions conditionally, $s_\theta^\star(X_t, t, \{(\mathbf{x}_i, y_i)\}_{i=1}^{N_i}) \approx \nabla_{X_t} \log p_t(X_t | \{(\mathbf{x}_i, y_i)\}_{i=1}^{N_i})$, which should be trainable in the same way as before, just using a split context/target set.
\end{itemize}


\subsection{Applications}
\cite{dupont2021Generative}




\section{Extension: Learning distributions on manifolds}
\todo[inline]{(Credits to Valentin and Arnaud for the original idea)}
We assume that we are given a compact Riemannian manifold $(\M, g)$, with a data distribution $p_0$ having support on $\M$.
We also assume $\M$ to be isometrically embedded in a Euclidean space, i.e.\ $\M \subset \R^d$, which always exists thanks to Nash embedding theorem (although it may be impractical in some cases).

\subsection{Forward process: perturbating data}

Let's construct the diffusion process $\{X(t)\}_{t=0}^T  \in \M$, as the solution to the following Stratonovich SDE %\footnote{\url{https://people.math.harvard.edu/~ctm/math219/home/sources/hsu.pdf}}\cite{elworthy1982Stochastic}:
% with $W$ a standard Wiener process, $f: \R^d \times \R \rightarrow \R^d$ a drift coefficient and $g: \R \rightarrow \R$ a diffusion coefficient.
\begin{align}
dX_t = P(X_t) \circ dB_t
\end{align}
with $B_t$ a standard Brownian process in the ambient Euclidean space and $P(X):\R^d \rightarrow T_{X}\M$ the projection onto the tangent space. For instance one has $P(X) \xi = \xi - \langle \xi, X \rangle X$, for $X \in \M = \mathbb{S}^n, \xi \in \R^{n+1}$.
%
\todo[inline]{Is it better or simpler to define the Brownian motion intrinsically?}
\mjh{probably? you can define Brownian motion as the process whose transition density function is the heat kernel on the manifold, which you get at via the Laplace-Beltrami operator. This will probably only matter if the time steps you do to approx the diffusion are large compared to the scale of the manifold.}
\emile{Yes, they define the same object, so the considerations around the time step concerns more the discretisation scheme.}

In local coordinates one can define a diffusion process as the solution of the Ito SDE
\begin{align}
dX_t =  G^{-1/2}(X_t) dB_t
\end{align}
with $G(X)$ the metric tensor in matrix form and $G^{-1/2}$ the (unique) symmetric square root of $G(X)^{-1}$.
%

\paragraph{Invariant prior}
Since we assume $\M$ to be compact, $\text{Vol}(\M)$ is finite so the uniform distribution with density (w.r.t. to the measure induced by the Riemmanian metric) $p \propto 1/\text{Vol}(\M)$ is proper.
It is invariant with respect to the Brownian diffusion.
% \todo[inline]{What is the invariant prior? Riemmanian vs Warped Gaussian?}
% The invariant prior is the Riemannian Gaussian which density (w.r.t. to the measure induced by the Riemmanian metric) is given by
% \begin{align}
% p(\x) \propto \exp\left\{ \frac{d_{\M}()^2}{} \right\}
% \end{align}

%
This process can be simulated as \cite{hairer2011Solving} %via a Euler-Maruyama discretisation:
\begin{align}
% X_{k+1} = (1 - \beta) X_{k} + Z_k, \quad Z_k \sim \N(0, 1).
% X_{k+1} = R_{X_{k}} \left( - \beta X_{k} + Z_k \right), \quad Z_k \sim \N(0, 1)
X_{k+1} = R_{X_{k}} \left(G^{-1/2}(X_k) Z_k \right), \quad Z_k \sim \N(0, I_d) \in T_{X_{k}}\M 
\end{align}
with $R_{X_k}$ a \emph{retraction}\footnote{A \emph{retraction} $R_{x}$ is defined as a map $R_{x}: T_{x}\M \rightarrow \M$ such that there exists $r>0$ s.t.\ $d(R_{x}(s\bm{v}),\exp_{x}(s\bm{v})) \le rs^2$ for $s$ sufficiently small, $\|\bm{v}\|=1$.}.
\emile{With $R_{x}=\exp_{x}$, $X_{k+1}$ is distributing according a \emph{Wrapped }Normal distribution (and not the \emph{Riemannian} normal, i.e. the maximum entropy Gaussian generalisation). This should not matter though.}
% \begin{align}
% % p(x(t+\delta)|x(t)) &\propto \exp \left\{ - \frac{\| \log_{x(t)}(x(t+\delta)) - f(x(t)) \|^2_{x(t)}}{2 \delta^2} \right\} \\
% p(x(t+\delta)|x(t)) &\propto \exp \left\{ - \frac{d_\M(x(t+\delta), x(t))^2}{2 \delta^2} \right\}  \\
%  &\propto \exp \left\{ -\frac{\| \log_{x(t)}(x(t+\delta)) \|^2_{x(t)}}{2 \delta^2} \right\}
% \end{align}


% \paragraph{Discretisation}

% \begin{align}
% p(x(t)|x(t+\delta)) 
% &= p(x(t+\delta)|x(t)) p(x(t)) / p(x(t+\delta)) \\
% &= p(x(t+\delta)|x(t)) \exp \left\{ \log  p(x(t)) - \log p(x(t+\delta)) \right\}  \\
% &\propto \exp \left\{ - \frac{\| \log_{x(t)+\delta)}(x(t) \|^2_{x(t+\delta)}}{2 \delta^2}  \right\} \times \exp \left\{ -\langle \nabla \log p(x(t+\delta),\log_{x(t)+\delta}(x(t)) \rangle_{x(t+\delta)} \right\} \\
% &\propto \exp \left\{ - \frac{\| \log_{x(t+\delta)}(x(t)) - 2 \delta^2 \nabla \log p(x(t+\delta)) \|^2_{x(t+\delta)}}{2 \delta^2}  \right\}
% \end{align}

\subsection{Reserve process: generating data}
Can we prove that the reverse-time process $(Y_t)_{t \in [0,T]} = (X_{T-t})_{t \in [0,T]}$ is also a diffusion? [PROOF?] which satisfies
%
\begin{align}
% dY_t = [-f(Y_t, T-t) - g(T-t)^2 \nabla \log p_{T-t}(Y_t)] dt + g(T-t) dW.
dY_t = \nabla \log p_{T-t}(Y_t) ~dt + P(X_t) \circ dB_t
\end{align}
with $\nabla: f \mapsto  G^{-1} df$ the (Riemannian) gradient.
%
This process can be simulated via
\begin{align}
X_{k} = R_{X_{k+1}} \left(G^{-1}(X_{k+1}) \nabla_E \log p_{k+1}(X_{k+1}) + G^{-1/2}(X_{k+1}) Z_{k+1} \right), \quad Z_{k+1} \sim \N(0, I_d) \in T_{X_{k+1}}\M \cong \R^d
\end{align}


\subsection{Training}
%
The score neural network is a function defined as $s_\theta: \M \times [0, T] \rightarrow T\M$.
It is a time-dependent vector-field.

\paragraph{Denoising score matching}

The DSM loss requires computing $\nabla_{X_t} \log p(X_t|X_0)$ which is non trivial for non-Euclidean manifolds.
% A closed-form expression may be known in some cases like $\mathbb{S}^d$ but it involves an infinite sum.

If $\mathcal{M}$ is a $d$-dimensional smooth compact manifold, the heat kernel can be expanded as a uniformly and absolutely convergent power series \citep{jones2008Manifold,li2019Variational}
\begin{align} \label{eq:heat_kernel}
p_t(x, y) = \sum^\infty_n e^{-t \lambda_n} \psi_n(x) \psi_n(y)
\end{align}
with $\{\lambda_n\}_n$ and $\{\psi_n\}_n$ respectively the eigenvalues and eigenfunctions of the Laplace-Beltrami operator $\Delta_\mathcal{M}$.
For instance with $\mathbb{S}^d$, we know \citep{borovitskiy2020Matern,devito2019Reproducing,zhao2018Exact} that $\lambda_n = n(n + d - 1)$ and $$\psi_n(x) \psi_n(y) = \frac{2n+d-1}{d-1} \frac{1}{A_{\mathbb{S}^n}} \mathcal{C}_n^{(d-1)/2}(x \cdot y)$$  where $\mathcal{C}_n^{(d-1)/2}$ are Gegenbauer polynomials.
An exact sampling scheme exists for $\mathbb{S}^d$ \cite{mijatovic2020note} but it is non trivial to implement \footnote{https://github.com/konkam/ExactWrightFisher.jl}.

When $d=2$, then the eigenfunctions are the spherical harmonics and the Gegenbauer polynomials are the Legendre polynomials $P_n$, we thus get \citep{jammalamadaka2019Harmonic,mardia2000Directional}: 
$$p_t(x, y) = \sum^\infty_{n=0} e^{- n(n+1) \cdot t } ~\frac{2n + 1}{4 \pi} P_n(x \cdot y).$$
When $d=1$, the heat kernel and Wrapped normal density coincide which means one can easily sample $X_t|X_0$.
Additionally, around $t \approx 0$, \cref{eq:heat_kernel} can be expended as
$$p_t(x, y) = (4\pi t)^{-d/2} G(r)^{-1/2} \exp \left(-\frac{r^2}{4t}\right) + \mathcal{O}(1)$$
with $r=d_\mathcal{M}(x,y)$.
Higher order expansions can be obtained \cite{rey2019diffusion,zhao2018Exact}.
One could get an unbiased estimator of \cref{eq:heat_kernel} via the Russian roulette estimator $\sum_n \Delta_n = \mathbb{E}_{N \sim p} \left[ \sum^N_n \frac{\Delta_n}{\mathbb{P}(N \ge n)} \right]$, although what we care in practice about $\nabla_x \log p_t(x, y)$ where the $\log$ would bias the estimator.



\paragraph{Implicit score matching}
We can alternatively rely on the \emph{implicit} or \emph{sliced} (which is an unbiased stochastic estimator of the former) score matching losses which are shown in \cref{tab:sm_losses}.
Relying on a particular choice of generator for the vector field can simplify the estimation of the divergence as discussed in the next paragraph.

\begin{table}[h]
\centering
\begin{tabular}{cc}
\toprule
Method                     & Loss \\ \hline
$\mathcal{L}_{\text{ESM}}$ &  $\frac{1}{2} \E \left[ \| \bm{s}_\theta(X_t, t) - \nabla \log p(X_t) \|^2_\Lambda \right]$    \\
$\mathcal{L}_{\text{DSM}}$ &  $\frac{1}{2} \E \left[ \| \bm{s}_\theta(X_t, t) + \nabla \log p(X_t|X_0) \|^2_\Lambda \right]$ \\
$\mathcal{L}_{\text{DSM??}}$ &  $\frac{1}{2} \E \left[ \| \bm{s}_\theta(X_t, t) + \nabla \log p(X_t|X_{t-dt}) \|^2_\Lambda \right]$ \\
$\mathcal{L}_{\text{ISM}}$ &  $\E \left[\frac{1}{2} \| \bm{s}_\theta(X_t, t)\|^2_\Lambda + \nabla \cdot \left(\Lambda^\top \bm{s}_\theta(X_t, t) \right) \right]$    \\
$\mathcal{L}_{\text{SSM}}$ &  $\frac{1}{2} \E \left[\frac{1}{2} \| \bm{s}_\theta(X_t, t)\|^2_\Lambda + v^\top \nabla \left(\Lambda^\top \bm{s}_\theta(X_t, t) \right) v \right]$    \\
\bottomrule
\end{tabular}
\caption{Score matching losses. $v$ follows the Rademacher distribution. Expectations are taken w.r.t.\ $p(X_t|X_0)$.}
\label{tab:sm_losses}
\end{table}


\paragraph{Parametrisation}
For $\M \neq \R^n$, it is non trivial to parametrise the output of $s_\theta(x, t)$ which is a tangent vector which lives in $T_{x}\M$ and this can be done in several ways.
%
\begin{itemize}
    \item \textbf{Projected vector field}: $s_\theta(x, t) = \text{proj}_{T_{x}\M}(\tilde{s}_\theta(x, t)) = P_x \tilde{s}_\theta(x, t) $ with $\tilde{s}_\theta(x, t) \in \R^d$ an ambient vector and $P_x$ the linear projection over the tangent space at $x$. Likely the easiest.
    According to \cite{rozen2021moser}, then $\text{div}(s_\theta(x, t)) = \text{div}_E(s_\theta(x, t))$, where $\text{div}_E$ denotes the standard Euclidean divergence.
    % \mjh{We use the same trick in the vec field GP paper} \emile{We also do that in practice in \cite{mathieu2020riemannian}}
    
    \item \textbf{Coordinates vector fields}: $s_\theta(\bf{z}, t) = \sum_i s^i_\theta(\bf{z}, t) f^i(\bf{z})$ with $f^i(\bf{z})$ the vector fields induced by a choice of local coordinates $x(\mz)$ with $\mz \in U \subset \R^d$. For the sphere, think of the sperhical coordinates $x(\mz)=x(\theta, \varphi))$ with $f^\theta = \partial x/ \partial_\theta$ and $f^\varphi = \partial x/ \partial_\varphi$.
    Then the divergence can be computed in these local coordinates: $\text{div}(s_\theta(x, t)) = \frac{1}{\sqrt{|\det g|}} \sum_i \frac{\partial}{\partial_i} \sqrt{|\det g|} s^i_\theta(\bf{z}, t)$. One recovers the standard divergence in spherical coordinates with this formula.
    If the manifold is not parallelisable, there does not exist a global frame, which implies that $\{f^i(x)\}$ is not a basis for any $x \in \M$. Think of the Hairy-ball theorem for the (n-)sphere.
    % \mjh{but we don't actually need a basis. We just need a smooth set of basis vector fields that span the tangent space. The fields can be redundant e.g. 3 axis fields on the sphere. We then just mix these smooth fields with smooth scalar coeffs from an nn and mix them to get a smooth field}
    % \emile{Agree that we don't need a basis, we only need a generator of the tangent bundle.}
    
    \item \textbf{Divergence-free vector fields}: $s_\theta(x, t) = \sum_i s^i_\theta(x, t) f^i(x)$  with $f^i(x)$ the vector fields induced by the isometries of $\M$ which are defined as $f_i(x) = \left.\frac{d}{dt}\right|_{t=0} \exp(t\,\xi_i)\cdot x$ (with some basis $(\xi_i\,;i=1,\ldots,n)$ of $T_{x}\M$ ). For any homogeneous space, this family of vector fields generatez the tangent bundle and is divergence-free.
    % \mjh{Is the divergence free nature of the field going to impose some extra constraint on the score function, and the resulting process?}
    % The $f^i$ are divergence free but the linear combination which yields $s_\theta$ has no constraint.
    Then we have $\text{div}(s_\theta(x, t)) = \sum_i \text{div}(s^i_\theta(x, t) f^i(x)) = \sum_i s^i_\theta(x, t) ~\text{div}(f^i(x)) + \langle \frac{\partial s^i_\theta(x, t)}{\partial x}, f^i(x)\rangle =  \sum_i \langle \frac{\partial s^i_\theta(x, t)}{\partial x}, f^i(x)\rangle$.
    This can similarly be estimated stochastically.
%    For more information see notes at \url{https://www.overleaf.com/read/thvfprqwmkjq}.
    % Not sure whether this can be useful here but this term does appear in standard (non-denoising) score matching \cite{hyvarinenEstimation,song2019Sliced}. }
\end{itemize}


\subsection{Applications}
Applications from \cite{mathieu2020riemannian,cohen2021riemannian,rezende2020Normalizing,rezende2021Implicit,falorsi2021Continuous}.


\section{Equivariance}
\todo[inline]{(Credits to Michael for the original idea)}
Following \cite{kohler2020Equivariant}, a probability density $p=p_0\circ\phi^{-1}$ (i.e.\ pushforward distribution) is invariant w.r.t.\ group action $\rho(g)$ for $g \in G$ if (sufficient condition) $p_0$ is invariant and $\phi$ is equivariant w.r.t. to G.

Given that diffusion processes satisfy the ODE given in \cref{eq:ode}, this implies that if $p_T$ is invariant and $\left[ f(X_t, t) - \frac{1}{2} g(t)^2 \nabla_{X_t} \log p_t(X_T) \right]$ is equivariant, then $p_t$ is invariant.

We have
\begin{align}
\left[f(\cdot, t) - \frac{1}{2} g(t)^2 \nabla_{\cdot} \log p_t(\cdot) \right] \left(\rho(g) X_t \right) 
&= \left[- \beta \rho(g) X_t  - \frac{1}{2} g(t)^2  \nabla_{\rho(g) X_t} \log p_t\left(\rho(g) X_t\right)) \right] \\
& \approx \left[- \beta \rho(g) X_t - \frac{1}{2} g(t)^2 s_\theta\left(\rho(g) X_t, t\right) \right]
\end{align}
assuming an Ornstein–Uhlenbeck process and plugging the score approximation.

It is then sufficient to parametrise the score network so that it is equivariant w.r.t. its first argument as we have that $\rho(g)$ and the drift commute, i.e.\
\begin{align}
    \left[- \beta - \frac{1}{2} g(t)^2 s_\theta\left(\cdot, t\right) \right] \left(\rho(g) X_t \right) = 
    \rho(g) \left[- \beta - \frac{1}{2} g(t)^2 s_\theta\left(\cdot, t\right)\right] (X_t).
\end{align}

\paragraph{Architecture}
In practice the score network $s_\theta$ could be parametrised with an EMLP \cite{finzi2021Practicala}.

\section{Exploring the hypercube!}
Equivariant stochatic vector fields on manifolds?

Application climate data \cite{holderrieth2021equivariant}: $\mathcal{P}_+^1\left(\mathcal{C}\left(\mathbb{S}^2, T\mathbb{S}^2\right)\right)$ with $G=SO(3)$.

\section{possibly some more useful refs}
\cite{song2020score} Score-based generative modelling through SDEs

% \url{https://link.springer.com/chapter/10.1007/978-1-4612-0209-7_6} Time reversal of diffusion on manifolds \url{https://d-nb.info/962215848/04}
% \emile{Do they give the reverse diffusion formula? I don't have access.}

\cite{oksendal2003stochastic} SDE book cited in the above

\cite{anderson1982reverse} Reverse time diffusion equations results

\cite{anonymous2022geodiff} - equivalence in diff processes for 3d molecules

\cite{anonymous2022pseudo} - RK methods for diffusion processes to stay closer to the data manifolds (?)

Statement:

For a Ito sde of the form $$dX_t = f(X_t, t) dt + g(X_t, t)dW_t$$, where $f: \R \to \R^n$, $g: \R \to \R^{n \times n}$, with certain smoothness and growth properties which guarantee existence and uniqueness of a solution (Anderson cites \cite{kushner1974stochastic} for this), and the existance of the probability density $p(X_t,t)$ for $t_0 \leq t\leq T$ as a smooth and unique solution of its associated Kolmonogorv eqn. Also define an $\bar{W}_t$ by $W_{t_0} = 0$ and $$d\bar{W}_t^i = dW_t^i + \frac{1}{p(X_t, t)} \sum_j \frac{\partial}{\partial X_t^j} [ p(X_t, t) g^{ji}(X_t, t)]dt $$ then (skipping some) a reverse time model for $X_t$ is defined by $$dX_t = \bar{f}(X_t, t) dt + g(X_t, t) d\bar{W}_t$$ where $$\bar{f}^i(X_t, t) = f^i(X_t, t) - \frac{1}{p(X_t, t)}\sum_{j, k} \frac{\partial}{\partial X_t^j}[p(X_t, t) g^{ik}(X_t, t)g^{jk}(X_t, t)]$$
\mjh{Translate above in to matrix maths and understand relation to the above}
$$dW_t = dW_t + \frac{1}{p(X_t, t)}\nabla \cdot [p(X_t, t) g(X_t, t)] = dW_t + \nabla \cdot g(X_t, t) + g(X_t, t) \nabla \log p(X_t, t)$$
\mjh{this $\bar{W}_t$ is a standard Weiner process / Browninan motion?}
$$\bar{f}(X_t, t) = f(X_t, t) - \frac{1}{p(X_t, t)} \nabla \cdot [p(X_t, t) g(X_t, t) g(X_t, t)^\top] = f(X_t, t) - \nabla \cdot[g(X_t, t)g(X_t, t)^\top] - g(X_t,t)g(X_t, t)^\top \nabla \log p(X_t, t)$$

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main_arxiv"
%%% End:
