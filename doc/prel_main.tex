\section{Euclidean Score-based Generative Modeling}
\label{sec:eucl-sgm-riem}

We recall here briefly the key concepts behind SGMs on the Euclidean space $\rset^d$ for some $d \in \nset$. We refer to \cite{song2020score,song2019generative,debortoli2021neurips} for a more detailed introduction to SGMs. In what follows, let $p_0$ denote the
data distribution. We have practically only access to an empirical approximation of this distribution given by the available data.


We consider a forward
\emph{noising} process $(\bfX_t)_{t \geq 0}$ defined by the following Stochastic
Differential Equation (SDE)
\begin{equation}\label{eq:forward_SDE}
  \rmd \bfX_t = -\bfX_t \rmd t + \sqrt{2} \rmd \bfB_t,\quad \bfX_0 \sim p_0 
\end{equation}
where $(\bfB_t)_{t \geq 0}$ is a $d$-dimensional Brownian motion. As a result
$(\bfX_t)_{t \geq 0}$ is an Ornstein--Ulhenbeck process targeting a multivariate standard Gaussian
distribution. Let $T \geq0$, under
mild conditions on the data distribution $p_0$, the time-reversed process
$(\bfhX_t)_{t \geq 0} = (\bfX_{T-t})_{t \in \ccint{0,T}}$ also satisfies an SDE
\citep{cattiaux2021time,haussmann1986time} given by
\begin{equation} \label{eq:backward_SDE}
  \rmd \bfhX_t = \{ \bfhX_t + 2 \nabla \log p_{T-t}(\bfhX_t)\} \rmd t + \sqrt{2} \rmd \bfB_t,\quad \bfhX_0 \sim p_T 
\end{equation}
where $p_t$ denotes the density of $\bfX_t$. By construction, the law of $\bfhX_{T-t}$ is equal to the law of $\bfX_t$ for $t \in \ccint{0,T}$ and in particular $\bfhX_{T}\sim p_0$. Hence, if one could sample from
$(\bfhX_t)_{t \in \ccint{0,T}}$ then its final distribution would be the target
data distribution $p_0$.  

Unfortunately there are three sources of intractability that prevents us from sampling the process $(\bfhX_t)_{t \in \ccint{0,T}}$. 

\textbf{Problem 1:} Its initial distribution is given by $p_T$ which is intractable.

\textbf{Solution:} The Ornstein--Ulhenbeck process \eqref{eq:forward_SDE} converges exponentially fast towards a standard multivariate Gaussian so one can approximate $p_T$ by this Gaussian for $T$ large enough. 

\textbf{Problem 2:} The scores are intractable so the dynamics \eqref{eq:backward_SDE} cannot be implemented. 

\textbf{Solution:} To approximate the scores, we exploit the following identity 
\begin{equation}\label{eq:scoreidentity}
  \textstyle{\nabla \log p_t(x) = \int_{\rset^d} \nabla \log p_{t|0}(x|x_0) p_{0|t}(x_0|x) \rmd x_0,}
\end{equation}
where $p_{t|0}(x'|x)$ is the transition density of the Ornstein--Ulhenbeck process which is available in closed-form. It follows directly that $\nabla \log p_t$ is the minimizer of the loss function
$\ell_t(s) = \expeLigne{\normLigne{s(\bfX_t) - \nabla \log
    p_{t|0}(\bfX_t|\bfX_0)}^2}$ over function $s$ where the expectation is over the joint distribution of $\bfX_0,\bfX_t$. This result can be exploited as follows. We consider a neural network approximation $\bm{s}_\theta: \ccint{0,T} \times \rset^d \to \rset^d$ which we train by minimizing the loss function $\ell(\theta)=\int_0^T \lambda_t \ell_{t}(\bm{s}_\theta(t,\cdot))\rmd t$ for some weighting function $\lambda_t>0$ . 


\textbf{Problem 3:} The loss function $\ell(\theta)$ and the SDE approximating \eqref{eq:backward_SDE} by replacing the scores $(\nabla \log p_t)_{t \in \ccint{0,T}}$ by $(s_\theta(t,\cdot))_{t \in \ccint{0,T}}$ and $p_T$ by the standard multivariate normal cannot not be simulated exactly on a computer.

\textbf{Solution:} For a discretization step $\gamma$ such that $T=\gamma N$ for integer $N$, the loss function is approximated by $\sum_{n=0}^N \lambda_{n\gamma} \ell_{n \gamma}(\bm{s}_\theta(n \gamma,\cdot))$ and we perform a Euler--Maruyama discretization of the resulting SDEe; i.e. we define $(Y_n)_{n \in \{0, \dots, N\}}$ such that for $Z_n\overset{\textup{i.i.d.}}{\sim} \mathcal{N}(0,I_d)$
% \begin{equation}
%   \label{eq:backward_discreteexactscores}
%   Y_{n+1} = Y_n + \gamma \{Y_n + 2 \nabla \log p_{T -n \gamma}(Y_n) \} + \sqrt{2} Z_{n+1},\quad Y_0\sim \mathcal{N}(0,I_d)
% \end{equation}
% for any $n \in \{0, \dots, N-1\}$ where $\mathcal{N}(0,I_d)$ denotes the multivariate standard normal on $\mathbb{R}^d$,  $\gamma > 0$ such that $T = N \gamma$, $(Z_n)\overset{\textup{i.i.d.}}{\sim} \mathcal{N}(0,I_d)$.
    
    
    
%     Finally, at sampling times we consider the following dynamics
\begin{equation}\label{eq:backward_discrete_final}
  Y_{n+1} = Y_n + \gamma \{Y_n + 2  \bm{s}_\theta(T -n \gamma, Y_n) \} + \sqrt{2} Z_{n+1},\quad Y_0\sim \mathcal{N}(0,I_d).
\end{equation}




% \textbf{Problem 2:} The continuous-time process \eqref{eq:backward_SDE} cannot not be simulated exactly on a computer even if the scores $(\nabla \log p_t)_{t \in \ccint{0,T}}$ were known. 

% \textbf{Solution:} We perform a time-discretization of the resulting SDE using an Euler--Maruyama scheme; i.e. we define $(Y_n)_{n \in \{0, \dots, N\}}$ such that
% \begin{equation}
%   \label{eq:backward_discreteexactscores}
%   Y_{n+1} = Y_n + \gamma \{Y_n + 2 \nabla \log p_{T -n \gamma}(Y_n) \} + \sqrt{2} Z_{n+1},\quad Y_0\sim \mathcal{N}(0,I_d)
% \end{equation}
% for any $n \in \{0, \dots, N-1\}$ where $\mathcal{N}(0,I_d)$ denotes the multivariate standard normal on $\mathbb{R}^d$,  $\gamma > 0$ such that $T = N \gamma$, $(Z_n)\overset{\textup{i.i.d.}}{\sim} \mathcal{N}(0,I_d)$.






% First, its initial distribution is given by $p_T$ which is intractable. Second, the continuous-time process \eqref{eq:backward_SDE} cannot not be simulated exactly on a computer even if the scores $(\nabla \log p_t)_{t \in \ccint{0,T}}$ were known. Third, the scores are also intractable. \emile{repetition with following paragraph} 

% Unfortunately there are three sources of intractability that prevents us from sampling the process $(\bfhX_t)_{t \in \ccint{0,T}}$. First, its initial distribution is given by $p_T$ which is intractable. However, the
% Ornstein--Ulhenbeck process \eqref{eq:forward_SDE} converges exponentially fast towards a standard multivariate Gaussian so one can approximate $p_t$ by this Gaussian for $T$ large enough. Second, the continuous-time process \eqref{eq:backward_SDE} cannot not be simulated exactly on a computer even if the scores $(\nabla \log p_t)_{t \in \ccint{0,T}}$ were known. 
% We then perform a time-discretization of the resulting SDE using an Euler--Maruyama scheme; i.e. we define $(Y_n)_{n \in \{0, \dots, N\}}$ such that
% \begin{equation}
%   \label{eq:backward_discreteexactscores}
%   Y_{n+1} = Y_n + \gamma \{Y_n + 2 \nabla \log p_{T -n \gamma}(Y_n) \} + \sqrt{2} Z_{n+1},\quad Y_0\sim \mathcal{N}(0,I_d)
% \end{equation}
% for any $n \in \{0, \dots, N-1\}$ where $\mathcal{N}(0,I_d)$ denotes the multivariate standard normal on $\mathbb{R}^d$,  $\gamma > 0$ such that $T = N \gamma$, $(Z_n)\overset{\textup{i.i.d.}}{\sim} \mathcal{N}(0,I_d)$.
% Third, the scores are also intractable so the discrete-time process \eqref{eq:backward_discreteexactscores} cannot be implemented either. To approximate the scores, we exploit the following identity 
% \begin{equation}\label{eq:scoreidentity}
%   \textstyle{\nabla \log p_t(x) = \int_{\rset^d} \nabla \log p_{t|0}(x|x_0) p_{0|t}(x_0|x) \rmd x_0,}
% \end{equation}
% where $p_{t|0}(x'|x)$ is the transition density of the Ornstein--Ulhenbeck process which is available in closed-form. It follows directly that $\nabla \log p_t$ is the minimizer of the loss function
% $\ell_t(s) = \expeLigne{\normLigne{s(\bfX_t) - \nabla \log
%     p_{t|0}(\bfX_t|\bfX_0)}^2}$ over function $s$ where the expectation is over the joint distribution of $\bfX_0,\bfX_t$. In practice, we thus consider consider a neural network approximation
% % $\bm{s}_\theta: \big\{0,...,N-1\big\} \times \rset^d \to \rset^d$ 
% $\bm{s}_\theta: [0, T] \times \rset^d \to \rset^d$ 
% which we train by minimizing over $\theta$ the loss 
% % $\sum_{n=0}^{N-1} \lambda_n \ell_{n \gamma}(\bm{s}^\theta(n \gamma, \cdot))$ for some positive weights $\lambda_n>0$ in a preliminary training phase.
% $ \mathbb{E}_{t}\left[\lambda(t) \ell_{t}(\bm{s}_\theta(t, \cdot))\right]$ where $t \sim \mathcal{U}([0,T])$ and $\lambda: [0, T] \rightarrow \R^{+}$ is a positive weighting function.
% Finally, at sampling times we consider the following dynamics
% \begin{equation}\label{eq:backward_discrete_final}
%   Y_{n+1} = Y_n + \gamma \{Y_n + 2  s_\theta(T -n \gamma, Y_n) \} + \sqrt{2} Z_{n+1},\quad Y_0\sim \mathcal{N}(0,I_d).
% \end{equation}

% \michael{Just playing with layout options}
% %% Testing enumerate
% Unfortunately there are three sources of intractability that prevents us from sampling the process $(\bfhX_t)_{t \in \ccint{0,T}}$. 

% \begin{enumerate}
%     \item Its initial distribution is given by $p_T$ which is intractable. 
%     However, the Ornstein--Ulhenbeck process \eqref{eq:forward_SDE} converges exponentially fast towards a standard multivariate Gaussian so one can approximate $p_t$ by this Gaussian for $T$ large enough. 
%     \item The continuous-time process \eqref{eq:backward_SDE} cannot not be simulated exactly on a computer even if the scores $(\nabla \log p_t)_{t \in \ccint{0,T}}$ were known. 
%     We then perform a time-discretization of the resulting SDE using an Euler--Maruyama scheme; i.e. we define $(Y_n)_{n \in \{0, \dots, N\}}$ such that
%     \begin{equation}
%       \label{eq:backward_discreteexactscores}
%       Y_{n+1} = Y_n + \gamma \{Y_n + 2 \nabla \log p_{T -n \gamma}(Y_n) \} + \sqrt{2} Z_{n+1},\quad Y_0\sim \mathcal{N}(0,I_d)
%     \end{equation}
%     for any $n \in \{0, \dots, N-1\}$ where $\mathcal{N}(0,I_d)$ denotes the multivariate standard normal on $\mathbb{R}^d$,  $\gamma > 0$ such that $T = N \gamma$, $(Z_n)\overset{\textup{i.i.d.}}{\sim} \mathcal{N}(0,I_d)$.
%     \item The scores are also intractable so the discrete-time process \eqref{eq:backward_discreteexactscores} cannot be implemented either.
%     To approximate the scores, we exploit the following identity 
%     \begin{equation}\label{eq:scoreidentity}
%       \textstyle{\nabla \log p_t(x) = \int_{\rset^d} \nabla \log p_{t|0}(x|x_0) p_{0|t}(x_0|x) \rmd x_0,}
%     \end{equation}
%     where $p_{t|0}(x'|x)$ is the transition density of the Ornstein--Ulhenbeck process which is available in closed-form. It follows directly that $\nabla \log p_t$ is the minimizer of the loss function
%     $\ell_t(s) = \expeLigne{\normLigne{s(\bfX_t) - \nabla \log
%         p_{t|0}(\bfX_t|\bfX_0)}^2}$ over function $s$ where the expectation is over the joint distribution of $\bfX_0,\bfX_t$. In practice, we thus consider consider a neural network approximation
%     $\bm{s}_\theta: \big\{0,...,N-1\big\} \times \rset^d \to \rset^d$. 
%       We train $\bm{s}_\theta$ by minimizing over $\theta$
%     $\sum_{n=0}^{N-1} \lambda_n \ell_{n \gamma}(\bm{s}^\theta(n \gamma, \cdot))$ for some positive weights $\lambda_n>0$ in a preliminary training phase. Finally, at sampling times we consider the following dynamics
%     \begin{equation}\label{eq:backward_discrete_final}
%       Y_{n+1} = Y_n + \gamma \{Y_n + 2  s^\theta_{T -n \gamma}(Y_n) \} + \sqrt{2} Z_{n+1},\quad Y_0\sim \mathcal{N}(0,I_d).
%     \end{equation}
% \end{enumerate}
% %% Testing enumerate

% %% Testing problem-solution
% Unfortunately there are three sources of intractability that prevents us from sampling the process $(\bfhX_t)_{t \in \ccint{0,T}}$. 

% \textbf{Problem 1:} Its initial distribution is given by $p_T$ which is intractable.

% \textbf{Solution:} The Ornstein--Ulhenbeck process \eqref{eq:forward_SDE} converges exponentially fast towards a standard multivariate Gaussian so one can approximate $p_t$ by this Gaussian for $T$ large enough. 


% \textbf{Problem 2:} The continuous-time process \eqref{eq:backward_SDE} cannot not be simulated exactly on a computer even if the scores $(\nabla \log p_t)_{t \in \ccint{0,T}}$ were known. 

% \textbf{Solution:} We perform a time-discretization of the resulting SDE using an Euler--Maruyama scheme; i.e. we define $(Y_n)_{n \in \{0, \dots, N\}}$ such that
% \begin{equation}
%   \label{eq:backward_discreteexactscores}
%   Y_{n+1} = Y_n + \gamma \{Y_n + 2 \nabla \log p_{T -n \gamma}(Y_n) \} + \sqrt{2} Z_{n+1},\quad Y_0\sim \mathcal{N}(0,I_d)
% \end{equation}
% for any $n \in \{0, \dots, N-1\}$ where $\mathcal{N}(0,I_d)$ denotes the multivariate standard normal on $\mathbb{R}^d$,  $\gamma > 0$ such that $T = N \gamma$, $(Z_n)\overset{\textup{i.i.d.}}{\sim} \mathcal{N}(0,I_d)$.

% \textbf{Problem 3:} The scores are also intractable so the discrete-time process \eqref{eq:backward_discreteexactscores} cannot be implemented either. 

% \textbf{Solution:} To approximate the scores, we exploit the following identity 
% \begin{equation}\label{eq:scoreidentity}
%   \textstyle{\nabla \log p_t(x) = \int_{\rset^d} \nabla \log p_{t|0}(x|x_0) p_{0|t}(x_0|x) \rmd x_0,}
% \end{equation}
% where $p_{t|0}(x'|x)$ is the transition density of the Ornstein--Ulhenbeck process which is available in closed-form. It follows directly that $\nabla \log p_t$ is the minimizer of the loss function
% $\ell_t(s) = \expeLigne{\normLigne{s(\bfX_t) - \nabla \log
%     p_{t|0}(\bfX_t|\bfX_0)}^2}$ over function $s$ where the expectation is over the joint distribution of $\bfX_0,\bfX_t$. In practice, we thus consider consider a neural network approximation
% $\bm{s}_\theta: \big\{0,...,N-1\big\} \times \rset^d \to \rset^d$. 
%   We train $\bm{s}_\theta$ by minimizing over $\theta$
% $\sum_{n=0}^{N-1} \lambda_n \ell_{n \gamma}(\bm{s}^\theta(n \gamma, \cdot))$ for some positive weights $\lambda_n>0$ in a preliminary training phase. Finally, at sampling times we consider the following dynamics
% \begin{equation}\label{eq:backward_discrete_final}
%   Y_{n+1} = Y_n + \gamma \{Y_n + 2  s^\theta_{T -n \gamma}(Y_n) \} + \sqrt{2} Z_{n+1},\quad Y_0\sim \mathcal{N}(0,I_d).
% \end{equation}
%% testing problem-solution

% Hence, we consider
% the process $(\bfY_t)_{t \in \ccint{0,T}}$ such that $\bfY_0$ is a Gaussian
% random variable with zero mean and identity covariance matrix and
% $(\bfY_t)_{t \in \ccint{0,T}}$ satisfies \eqref{eq:backward_SDE}. In order to
% obtain an algorithm which can be implemented in practice we first discretize the
% process \eqref{eq:backward_SDE}, \ie \ we define $(Y_n)_{n \in \{0, \dots, N\}}$
% such that $Y_0$ is a Gaussian random variable with zero mean and identity
% covariance matrix and for any $n \in \{0, \dots, N-1\}$ we have
% \begin{equation}
%   \label{eq:backward_discrete}
%   Y_{n+1} = Y_n + \gamma \{Y_n + 2 \nabla \log p_{T -n \gamma}(Y_n) \} + \sqrt{2} Z_{n+1} \eqsp ,
% \end{equation}
% where $\gamma > 0$ such that $T = N \gamma$, $(Z_n)_{n \in \nset}$ is a sequence
% of i.i.d. Gaussian random variables with zero mean and identity covariance
% matrix. Note that \eqref{eq:backward_discrete} is simply the Euler-Maruyama
% discretization of \eqref{eq:backward_SDE}. One last key step relies in the
% approximation of the dynamics of \eqref{eq:backward_discrete} since the
% logarithmic gradient (or Stein score) $(\nabla \log p_t)_{t \in \ccint{0,T}}$ is
% not tractable. To do so, we consider a neural network approximation
% $\bm{s}_\theta: \ \ccint{0,T} \times \rset^d \to \rset^d$. Since for any
% $t \in \ccint{0,T}$ and $x \in \rset^d$ we have that
% \begin{equation}
%   \textstyle{\nabla \log p_t(x) = \int_{\rset^d} \nabla \log p_{t|0}(x|x_0) p_{0|t}(x_0|x) \rmd x_0 \eqsp , }
% \end{equation}
% we obtain that for any $t \in \ccint{0,T}$, $\nabla \log p_t$ is the minimizer
% of the loss function
% $\ell_t(s) = \expeLigne{\normLigne{s(\bfX_t) - \nabla \log
%     p_{t|0}(\bfX_t|\bfX_0)}^2}$. We train $\bm{s}_\theta$ to minimize
% $\int_0^T \lambda(t) \ell_t(\bm{s}_\theta(t, \cdot)) \rmd t$, where $t \mapsto \lambda(t)$ is some positive weighting function.
% \emile{Would remove from main paper}
% In \citet[Theorem 1]{debortoli2021neurips}, error bounds on
% the total variational norm between the law of $Y_N$ and $p_0$ are established depending on $N$, $\gamma$
% and the approximation error of the neural network $\bm{s}_\theta$. In
% particular, the approximation error can be decomposed into two terms: one corresponding to the mixing of the process  \eqref{eq:backward_discreteexactscores} which is controlled by the mixing of the forward process \eqref{eq:forward_SDE}, another one which corresponds to the approximation of the scores. %Hence the bottleneck of SGMs are not mixing issues does not reside in the mixing time of the backward chain but in the approximation of the score.

We have presented the basics of SGM but we highlight that many recent works
improve on these models;
\citep[see e.g.][]{song2020score,song2020improved,song2020denoising,jolicoeur2020adversarial,jolicoeur2021gotta,nichol2021beatgans}. In
particular, it is worth noting that choosing an adaptive stepsize
$(\gamma_n)_{n \in \nset}$ \citep{bao2022analyticdpm,watson2021learning} drastically improve the
synthesis results as well as using a predictor-corrector scheme
\citep{song2020score} instead of a simple Euler--Maruyama discretization. Finally,
we note that there exist other approaches to introduce SGMs using variational and maximum likelihood
techniques \citep{ho2020denoising,huang2021variational,durkan2021maximum}.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
