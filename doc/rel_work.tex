\section{Related work}
\label{sec:related-works}

% The study of distribution on manifolds and their approximation is a central part
% of \emph{directional statistics}, see \citep{mardia2009directional}.
% Traditional methods rely on fitting mixtures of distributions to the target. However, in
% recent years several other methods have been introduced in the context of generative modeling.
In what follows, we discuss previous work on parametrizing family of distributions for manifold-valued data. %supported on manifolds.
% We first briefly discuss
Note that in this work, the manifold structure is considered to be prescribed.
In contrast, another line of work has been focusing on jointly learning the manifold structure and a generative model
% combining manifold learning and generative modeling have also been proposed 
\citep{brehmer2020flows,kalatzis2021multi,caterini2021Rectangular}.
% At a high level, the related work can be divided 


\paragraph{Parametric family of distributions.} Defining flexible easy-to-sample
distributions on manifolds is not a trivial task.
The various parametric families of distributions that have been proposed can broadly be categorised into three main approaches \citep{navarro2017multivariate}: wrapping, projecting and conditioning.
Wrapped distributions consider a parametric distribution on $\mathbb{R}^n$ that is pushed-forward along an invertible map $\psi: \mathbb{R}^n \rightarrow \M$.
% Parametric wrapped distributions are usually defined on $\mathbb{S}^1$, 
A canonical example is the wrapped normal distribution on $\mathbb{S}^1$
\citep{collett1981Discriminating}.  Another example has been proposed by
\cite{mathieu2019continuous,nagano2019wrapped} on the hyperbolic space with the
exponential map \valentin{this is the same thing as push forward of Euclidean
  NF?}.  Given a Euclidean submanifold $\M \subset \mathbb{R}^n$ and a
distribution $p_{\text{amb}} \in \Pens(\mathbb{R}^n)$,
% a distribution can be defined on $\M$ by marginalizing out a 
marginalizing out $p_{\text{amb}}$ along the normal bundle induces a distribution on $\M$.
Samples are obtained by first sampling $p_{\text{amb}}$ and then applying an orthogonal projection on these samples.
Finally, the conditioning method consists into considering the unormalized density defined by the restriction of an ambient density $p_{\text{amb}}$ with $\M$.
% According to \citep{navarro2017multivariate}, there exists three main methods to sample from
% these distributions on manifolds such as spheres and tori: wrapping, projecting and conditioning.
% Wrapped distributions are usually defined on $\mathbb{S}^1$,
% by considering a distribution on $\rset$. Then using that
% $\phi: \mathbb{S}^1 \times \zset \to \rset$ with
% $\phi(\theta, k) = \theta + 2k\uppi$ is a bijection, we define a probability
% distribution on $\mathbb{S}^1$ by marginalizing along the second component.
% Let $\mathcal{M}$ be a submanifold of $\rset^d$ such that
% $\mathcal{M} = \ensembleLigne{x \in \rset^d}{\phi^{-1}(x)_2 = a}$ with
% $\phi: \ \mse_1 \times \mse_2 \to \rset^d$ a diffeomorphism and $a \in
% \mse_2$
% Then, the projecting method consists into considering a probability
% distribution in $\mse_1 \times \mse_2$ and marginalizing w.r.t. $\mse_2$.
% On the
% $(d-1)$-dimensional sphere this amounts to sampling from the probability
% distribution and normalizing the samples.
% Finally, the conditioning method
% consists into considering the disintegration of a probability distribution
% w.r.t. $\phi^{-1}_2$.
Such distributions encompass the von Mises-Fisher
distribution \citep{fisher1953dispersion} and the Kent distribution
\citep{kent1982fisher}.
These distributions are usually unimodal and 
% in order to fit more complex distributions it is necessary to consider mixtures
considering mixtures of thereof is key to increase flexibility
\citep{peel2001fitting,mardia2008multivariate}.
% quid Riemannian normal distributions? (max entropy generalisation)
% also quid power spherical distribution?

% \paragraph{Normalizing flows in latent spaces.}
\paragraph{Push-forward of Euclidean normalizing flows.}
More recently, approaches leveraging the flexibility of normalizing flows
\citep{papamakarios2019normalizing} have been proposed.
Following the wrapping method described above, these methods 
% The simplest approach is to 
parametrize a normalizing flow in the Euclidean space $\mathbb{R}^n$ that is pushed-forward along an invertible map $\psi: \mathbb{R}^n \rightarrow \M$.
However, to globally represent the manifold, the map $\psi$ needs to be a homeomorphism, which can only happen if $\M$ is topologically equivalent to $\mathbb{R}^n$, hence limiting the scope of that approach.
One natural choice for this map if the exponential map $\exp_x: \mathrm{T}_x \M \cong \mathbb{R}^d$. %, leading so called wrapped distributions.
This approach has been taken, for instance, by \cite{falorsi2019reparameterizing} and \cite{bose2020latent}, respectively parametrizing distributions on Lie groups and hyperbolic space.
% \cite{gemici2016normalizing} introduced normalizing flows on the sphere using the stereographic projection.
% One limitation of this approach is
% that the probability distribution is hard to model near the pole which is sent
% to $\infty$ using the stereographic mapping. Indeed, if one tries to model a
% probability distribution with one mode near the pole then most of the mass of
% the distribution pushed by the stereographic mapping is concentrated away from
% the origin. As a result, it is hard to approximate this distribution by learning
% a deformation of an easy-to-sample distribution, like a Gaussian
% distribution.
% For Lie groups, \cite{falorsi2019reparameterizing} proposed to
% perform the inference in the Lie algebra and then push the distribution to the
% whole manifold using that for compact Lie groups the exponential mapping is
% surjective.
% Similarly, \cite{bose2020latent} proposed in hyperbolic spaces two
% approaches to push a normalizing flow defined in tangent spaces of the manifold
% using different wrappings.
%
% a normalizing flow approach based on a recursive construction which is more numerically stable than the one proposed in \citep{gemici2016normalizing}.

\paragraph{Neural ODE on manifolds.}
To avoid artifacts or numerical instabilities due to the manifold embedding, another line
of work uses tools from Riemannian geometry to define flows directly on the
manifold of interest
\citep{falorsi2020neural,mathieu2020riemannian,falorsi2021Continuous}.
Since these methods do not require a specific embedding mapping, they 
% can be considered as \emph{intrinsic}.
are referred as \emph{Riemannian}.
% These methods leverage tools from continuous normalizing flows (CNFs) \citep{grathwohl2019Scalable},
They extend continuous normalizing flows (CNFs) \citep{grathwohl2019Scalable} to the manifold setting, by implicity parametrizing flows as solutions of Ordinary Differential Equations (ODEs).
As such, the parametric flow is a \emph{continuous} function of time.
% extending the evolution equation of CNFs to Riemannian manifolds.
This approach has recently been extended by \cite{rozen2021moser}
introducing Moser flows, whose main appeal being that it circumvents the need to solve an ODE in the training process. % by reparametrizing the vector field with an interpolant function.
% is that they do not require backpropagating
% through and Ordinary Differential Equation (ODE).
% Similarly to Moser flow, RGSM learns an interpolation between the target distribution and an easy-to-sample distribution. 


% \begin{itemize}
% \item limitation 1: divergence to compute high dimensional
% \item limitation 2: the importance sampling (high dimensional as well)
% \item comparison on high dimensional sphere
% \item comparison with bunny as well
% \end{itemize}

\paragraph{Optimal transport on manifolds.}
Another line of work has focused on developing flows on manifolds 
% Finally, another recent method introduces flows on manifolds 
using tools from optimal transport. % \citep{ambrosio2003Optimal}.
\cite{sei2013jacobian} introduced a flow that is given by $f_\theta: x \mapsto \exp_x(\nabla \psi^c_\theta)$ 
% the exponential map applied to the gradient of a $c$-convex function, where $c$ is the squared distance on the Riemannian manifold.
with $\psi^c_\theta$ a $c$-convex function and $c=d^2_\M$, where $d_\M$ is the
geodesic distance.  This approach is motivated by the fact that the
optimal transport map takes such an expression
\citep{ambrosio2003Optimal}.  These methods operate directly on the manifold,
similarly to CNFs, yet in contrast they are \emph{discrete} in time.  The
benefits of this approach depend on the specific choice of parametric family of
$c$-convex functions \citep{rezende2021Implicit,cohen2021riemannian},
trading-off expressively with scalability.
% The optimization of these flows is then
% conducted on the parameters of the chosen family of $c$-convex functions
In the case of tori and spheres, \cite{rezende2020Normalizing} introduced \emph{discrete} Riemannian ﬂows based on Möbius transformations and spherical splines.


% Methods to check:
% \begin{itemize}
% \item \cite{mathieu2019continuous} -- pas vraiment relie. C'est le latent space d'un VAE qui est un espace hyperbolique.
% \item \cite{nagano2019wrapped} -- pareil ?
% \item \cite{rey2019diffusion} -- vae aussi
% \item \cite{falorsi2018explorations} -- aussi
% \item \cite{davidson2018hyperspherical} -- aussi
% \end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
