\section{Riemannian Score-based Generative Modeling}
\label{sec:score-appr-manif}

Similarly to the Euclidean setting, three ingredients are required to extend SGM
to compact Riemannian manifolds:
\begin{enumerate*}[label=\roman*)]
\item a forward \emph{noising} process on the Riemannian manifold which converges to an easy-to-sample reference distribution, 
\item a time-reversal formula on Riemannian manifolds which defines a backward generative process, 
\item a method to efficiently approximate the drift of the time-reversal process.
\end{enumerate*}
We address all these problems and introduce RGSM. The key differences between
SGMs and RSGMs are summarised in \cref{tab:difference}.


\begin{table}[h]
\small
\centering
\renewcommand*{\arraystretch}{1.2}
\begin{tabular}{lcc}
%   \toprule 
  Ingredient \textbackslash ~Space  &       Euclidean                & Compact manifold \\ \hline
  Forward process & Ornstein--Ulhenbeck & Brownian motion \\
  Easy-to-sample distribution & Gaussian & Uniform \\
  Time reversal  &  \citet[Theorem 4.9]{cattiaux2021time} & \cref{thm:time_reversal_manifold}  \\   
  Sampling of the forward process & Direct & Geodesic Random Walk \\
  Sampling of the backward process & Euler--Maruyama & Geodesic Random Walk \\
%   \bottomrule
\end{tabular}
\caption{\small Differences between SGM on Euclidean spaces and RSGM on compact Riemannian manifolds.}
\label{tab:difference}
\end{table}



\subsection{Brownian motion on compact Riemannian manifolds}
\label{sec:brown-moti-comp}

\paragraph{Brownian motion and uniform distribution}

First, we define a forward noising process on $\M$ targeting an easy-to-sample
reference distribution. In Euclidean spaces, the reference distribution is a
standard normal in the compact manifold setting the uniform distribution
$\piinv$ is the natural choice.  For most manifolds of interest, one can either
sample exactly from $\piinv$ or obtain approximate samples with high accuracy.
For the forward noising dynamics, the Ornstein--Ulhenbeck process
\eqref{eq:forward_SDE} used in Euclidean scenarios is now replaced by the
Brownian motion defined on $\M$ as it converges exponentially fast to
$\piinv$---see \Cref{prop:brownian_conv} below. We refer to
\Cref{sec:brown-moti-manif} for a general introduction to Brownian motions on
manifolds. This Brownian motion is defined as follows.

\begin{definition}[Brownian motion]
  $(\bfB^\M_t)_{t \geq 0}$ is a Brownian motion on $\M$ if
  $(\bfB^\M_t)_{t \geq 0}$ is associated with the SDE with infinitesimal
  generator $\generator(f) = \Delta f$, see \cref{sec:notation}.
\end{definition}

We refer to \cref{sec:brown-moti-manif} or \citet[Chapter 1, Chapter
3]{hsu2002stochastic} for the definition of a $\M$-valued semimartingale and the
Laplace-Beltrami operator. By \citet[Proposition 3.2.1]{hsu2002stochastic}, we
have that for any initial condition $\bfB^\M_0$ with distribution
$\pizero \in \Pens(\M)$, there exists $(\bfB^\M_t)_{t \geq 0}$. The convergence
rates are obtained w.r.t. the total variation distance between the uniform
distribution and the semi-group $(\Pker_t)_{t \geq 0}$\footnote{We define
  $(\Pker_{t,s})_{t, s \geq 0, t \geq s}$ the semi-group such that for any
  $f,g \in \rmc(\M)$ and $t, s \geq 0$ with $t \geq s$ we have
  $\expeLigne{f(\bfB_t^\M)g(\bfB_s^\M)} = \expeLigne{\int_{\M}f(y)
    \Pker_{t|s}(\bfB_s^\M, \rmd y) g(\bfB_s^\M)}$. For the rest of this paper we
  denote $(\Pker_t)_{t \geq 0} = (\Pker_{t|0})_{t \geq 0}$.}
%, see \citet[Proposition 2.6]{urakawa2006convergence}.

\begin{proposition}[{{Convergence of Brownian motion \cite[Proposition 2.6]{urakawa2006convergence}}}]
  \label{prop:brownian_conv}
  For any $t > 0$, $\Pker_t$ admits a density $p_t$ w.r.t $\piinv$ and
  $\piinv \Pker_t = \piinv$, \ie \ $\piinv$ is an invariant measure for
  $(\Pker_t)_{t \geq 0}$. In addition, if there exists $C, \alpha \geq 0$ such
  that for any $t \in \ocint{0,1}$, $p_t(x,x) \leq C t^{-\alpha /2}$ then 
  for any $\pizero \in \Pens(\M)$ and for any $t \geq 1/2$ we have 
  \begin{equation}
    \textstyle{\tvnorm{\pizero \Pker_t - \piinv} \leq C^{1/2} \rme^{\lambda_1 /2} \rme^{-\lambda_1 t}  ,}
  \end{equation}
  where $\lambda_1$ is the first non-negative eigenvalue of $-\Delta_\M$  in $\mathrm{L}^2(\piinv)$.
\end{proposition}

The diagonal upper bound on the heat kernel is satisfied for many manifolds
including the $d$-dimensional torus and sphere \cite[see][Section
3]{saloff1994precise}. Hence, \cref{prop:brownian_conv} ensures that under mild
conditions the Brownian motion converges exponentially fast towards the uniform
distribution on the compact Riemmanian manifold $\M$. Therefore, in the context
of SGM, the Brownian motion on $\M$ is the counterpart to the Ornstein--Ulhenbeck
process and the uniform distribution is the counterpart to the Gaussian one.

We note that in previous works on SGMs, the Brownian motion has also been used
as a forward noising process \citep{song2020score,song2019generative}. However,
in these cases, the Brownian motion is not geometrically ergodic and does not
admit any invariant distribution contrary to our setting. Two issues remain to
be solved. First, we need to be able to sample this forward noising process
$(\bfB_t^\M)_{t \geq 0}$. Second we need to obtain tractable approximations of
the heat kernel, i.e. the transition kernel of this process, in order to define
efficient score approximation schemes.

\paragraph{Sampling from diffusions}
In Euclidean spaces, sampling an Ornstein--Ulhenbeck process is straightforward
whereas obtaining samples from a Brownian motion on a manifold is non-trivial in
general. First, if $\M$ is isometrically embedded into $\rset^p$ (with
$p \geq d$)---i.e.\ $\M \subset \rset^p$---then we have that
$(\bfB^\M_t)_{t \geq 0}$ (seen as a process on the ambient space $\rset^p$)
satisfies the following SDE
\begin{equation}
  \label{eq:brownian_motion_extrinsic}
  \textstyle{\rmd \bfB^\M_t = \sum_{i=1}^p P_i(\bfB^\M_t) \circ \rmd \bfB_t^i,}
\end{equation}
where $\circ$ denotes the Stratanovitch integral \footnote{Manifold valued processes are usually defined using the Stratanovitch integral because it satisfies the chain rule of differential calculus. For more details we refer to \cref{sec:stoch-diff-equat}.},
$(\{\bfB_t^i\}_{i=1}^p)_{t \geq 0}$ is a $p$-dimensional Brownian motion and for
any $i \in \{1, \dots, p\}$ we have $P_i(x) = P(x) e_i$ for any $x \in \M$, where $\{e_i\}_{i=1}^p$ is the canonical basis of $\rset^p$
and $P(x): \ \rset^p \to \mathrm{T}_x \M$ is the orthogonal projection operator,
see \cref{sec:metr-conn-tens}. However, this approach is \emph{extrinsic} and
requires the knowledge of the projection
operator. % It is also limited to the Brownian motion and it is not
% easily extended to other diffusions on $\M$.
Here we consider an \emph{intrisic} 
approach based on Geodesic Random Walks (GRWs), see \cite{jorgensen1975central}
for a review of their properties.

GRWs are not restricted to approximating the Brownian motion on $\M$ but in fact
can approximate \emph{any} well-behaved diffusion on $\M$. This property will be
useful when sampling the backward diffusion process. Hence, we introduce GRWs in
a general framework and we are going to consider a discrete-time process
$(X_n^\gamma)_{n \in \nset}$ which approximates $(\bfX_t)_{t \geq 0}$
is associated with
\begin{equation}
  \label{eq:generic}
 \rmd \bfX_t = b(\bfX_t) \rmd t + \sigma(\bfX_t) \rmd \bfB_t^\M.
\end{equation}
Let
$\{ \nu_x \}_{x \in \M}$ such that for any $x \in \M$,
$\nu_x \in \Pens(\mathrm{T}_x \M)$. Assume that for any $x \in \M$,
$\int_{\M} \normLigne{v}^2 \rmd \nu_x(v)< +\infty$. In addition assume that
there exists $b \in \XM$ and $\Sigma \in \XMdeux$, such that for any $x \in \M$,
$\int_{\M} v \rmd \nu_x(v) = b(x)$ and
$\int_{\M} (v - b(x)) \otimes (v - b(x)) \rmd \nu_x(v) =
\Sigma(x) = \sigma(x) \sigma(x)^\top$. % In addition, we assume
    % that for any $x \in \M$, $\Sigma(x) = \mu^{(2)}(x) - \mu^{(1)}(x) \otimes \mu^{(1)}(x)$ is
    % strictly positive definite and that there exists $\Ltt \geq$ such that for
    % any $x, y \in \M$, $\tvnorm{\nu_x - \nu_y} \leq \Ltt d(x,y)$. Where we have
    % that for any $\nu_1 \in \mathrm{T}_x \M$ and $\nu_2 \in \mathrm{T}_y \M$,
    % \begin{equation}
    %   \tvnorm{\nu_x - \nu_y} = \sup \ensembleLigne{\nu_1[f] - \Gamma_{x}^y(\gamma)_\# \nu_2[f]}{\gamma \in \mathrm{Geo}_{x,y}, \ f \in \rmc(\mathrm{T}_x \M)}  . 
    % \end{equation}
    % Note that if $d(x,y) \leq \vareps$ then for some $\vareps > 0$ we have that $\abs{\mathrm{Geo}_{x,y}}=1$.


\begin{definition}[Geodesic Random Walk]
  Let $X_0$ be a $\M$-valued random variable.  For any $\gamma > 0$, we
  define $(\bfX_t^{\gamma})_{t \geq 0}$ such that $\bfX_0^\gamma = X_0$ and
  for any $n \in \nset$ and $t \in \ccint{0, \gamma}$,
  $\bfX_{n\gamma + t} = \exp_{\bfX_{n \gamma}}\left(t\gamma \{ \mu_n +
  (1/\sqrt{\gamma}) (V_n - \mu_n)\}\right)$\footnote{where $\exp_x: \ \mathrm{T}_x \M \to \M$ is the exponential mapping on the manifold, see \citet[Chapter 20]{lee2013smooth} for details.}, where $(V_n)_{n \in \nset}$ is a
  sequence of random variables in such that for any $n \in \nset$, $V_n$ has
  distribution $\nu_{\bfX_{n \gamma}}$ conditionally to $\bfX_{n
    \gamma}$. We say that
  $(X_n^\gamma)_{n \in \nset} = (\bfX_{n \gamma})_{n \in \nset} \in 
  \M$ is a Geodesic
  Random Walk (GRW).
\end{definition}

Note that for any $n \in \nset$ and $\gamma >0$, $X_n^\gamma \in \M$.  For any
$\gamma>0$, we denote by $(\Qker_n^{\gamma})_{n \in \nset}$ the sequence of
Markov kernels such that for any $n \in \nset$, $x \in \M$ and
$\msa \in \mcb{\M}$ we have that
$\updelta_x \Qker_n^\gamma(\msa) = \Pbb(X_n^\gamma \in \msa)$, with
$X_0^\gamma = x$.  GRWs are appealing because, under mild conditions, when the
stepsize $\gamma \to 0$, they converge towards $(\bfX_t)_{t \geq 0}$ solution of
\eqref{eq:generic} in the following sense:

\begin{theorem}[{{Convergence of geodesic random walk \cite[Theorem 2.1]{jorgensen1975central}}}]
  \label{thm:grw_diffusion}
  Under the conditions of \cref{thm:jorgensen_appendix}, for any $t \geq 0$,
  $f \in \rmc(\M)$  we have that
  $\lim_{\gamma \to 0} \normLigne{ \Qker_{\gamma}^{\ceil{t/\gamma}}[f] -
    \Pker_t[f]}_{\infty} = 0$, where $(\Pker_t)_{t \geq 0}$ is the
  semi-group associated with the infinitesimal generator
  $\generator: \ \rmc^\infty(\M) \to \rmc^\infty(\M)$ given for any
  $f \in \rmc^\infty(\M)$ by
  $\generator(f) = \langle b, \nabla f \rangle_{\M} + (1/2) \langle
  \Sigma, \nabla^2f \rangle_{\M}$.
\end{theorem}   

In particular if $b = 0$ and $\sigma = \Id$, then the random walk
converges towards a Brownian motion on $\M$ in the sense of the convergence
of semi-groups.
% In this case, for any $x \in \M$ in local coordinates we
% have that $\Phi_\# \nu_x$ has zero mean and covariance matrix $G(x)$, where
% $\Phi$ is a local chart around $x$ and
% $G(x) = (g_{i,j}(x))_{1 \leq i,j \leq d}$ the coordinates of the metric in
% that chart.
One advantage of GRW is that they allow to samples from arbitrary diffusions
under mild assumptions. This property will be key to sample from the backward
process. \cref{thm:grw_diffusion} can be extended to approximate time
inhomogeneous diffusions. We leave the proof of this extension for future
work. In \cref{alg:grw}, we remind how to approximately sample from a diffusion
$(\bfX_t)_{t \in \ccint{0,T}}$ using GRWs, where $(\bfX_t)_{t \in \ccint{0,T}}$
associated with the family of infinitesimal generator
$(\generator_t)_{t \in \ccint{0,T}}$ given for any $t \in \ccint{0,T}$ and
$f \in \rmc^2(\M)$ by
$\generator_t(f) = \langle b_t, \nabla f \rangle + \langle \Sigma_t, \nabla^2 f
\rangle$, where $b: \ \ccint{0,T} \to \XM$, $\Sigma_t = \sigma_t \sigma_t^\top$
with $\sigma_t : \ \ccint{0,T} \to \XMdeux$. For simplicity, in \cref{alg:grw},
we assume that $\M$ is embedded in $\rset^p$ and use the projection to define
the noise on the tangent space (such an embedding always exists using the Nash
embedding theorem, see \cite{gunther1991isometric} for example).  In a more
general setting, we compute the noise on the tangent space using local
coordinates.


\begin{algorithm}[!t]
\caption{\small Geodesic Random Walk (GRW)}
\label{alg:grw}
\begin{algorithmic}[1]
 \small
  \Require $T, K, X_0, b, \sigma, P$
  \State $\gamma = T / K$ \Comment Step-size
  \For{$k \in \{0, \dots, K-1\}$}
  \State $\bar{Z}_{k+1} \sim \mathcal{N}(0, I_p)$ \Comment Standard Gaussian in ambient space $\rset^p$
  \State $Z_{k+1} = P(X_k) \bar{Z}_{k+1}$ \Comment Projection in the tangent space $\mathrm{T}_x \M$ 
  \State $V_{k+1} = \gamma b(k \gamma, X_k) + \sqrt{\gamma} \sigma(k \gamma, X_k) Z_{k+1}$ \Comment Euler-Maruyama step on tangent space 
  \State $X_{k+1} = \exp_{X_k}\left(V_{k+1}\right)$ \Comment Geodesic projection onto $\M$
  \EndFor
  \State {\bfseries return} $\{ X_k\}_{k=0}^{K-1}$
\end{algorithmic}
\end{algorithm}

\paragraph{Heat kernel on compact Riemannian manifolds}
The semi-group of the Brownian motion $(\Pker_t)_{t \geq 0}$ (called the heat
kernel) admits a density w.r.t.\ $\piinv$, such that for any $f \in \rmc(\M)$,
$x_0 \in \M$ and $t > 0$ we have
\begin{equation}
  \textstyle{
    \updelta_{x_0} \Pker[f] = \int_{\M} f(x_t)p_{t|0}(x_t|x_0)  \rmd \piinv(x_t).
    }
\end{equation}
In addition, this transition density is positive and
$(t,x,y) \mapsto p_{t|0}(y|x) \in \rmc^\infty(\ooint{0,+\infty} \times \M \times
\M)$ and satisfies the heat equation $\partial_t p_{t|0} = \Delta
p_{t|0}$. However, contrary to the Gaussian transition density of the
Ornstein--Ulhenbeck process, it is typically only available as an infinite
series. In order to circumvent this issue we consider two
techniques: \begin{enumerate*}[label=\roman*)]
\item a truncation approach, 
\item a Taylor expansion around $t=0$, \ie \ a Varadhan asymptotics.
\end{enumerate*}    
%
First, we recall that in the case of compact manifolds we have that for any
$t > 0$ and $x, y \in \M$
\begin{equation}
  \label{eq:infinite_sum}
  \textstyle{p_{t|0}(x,y) = \sum_{j \in \nset} \rme^{-\lambda_j t} \phi_j(x)\phi_j(y),}
\end{equation}
where the convergence occurs in $\mathrm{L}^2(\piinv \otimes \piinv)$,
$(\lambda_j)_{j \in \nset}$ and $(\phi_j)_{j \in \nset}$ are the
eigenvalues, respectively the eigenvectors, of $-\Delta_\M$ in
$\mathrm{L}^2(\piinv)$ \cite[see][Section 2]{saloff1994precise}. When the eigenvalues and eigenvectors are known, we approximate the
logarithmic gradient of $p_{t|0}$ by truncating the sum in
\cref{eq:infinite_sum} with $J \in \nset$ terms to obtain for any
$t > 0$ and $x,y \in \M$
\begin{equation}
  \nabla_x \log p_{t|0}(x,y) \approx \textstyle{S_{J,t}(x,y) = \sum_{j=0}^J \rme^{-\lambda_j t} \nabla \phi_j(x) \phi_j(y) / \sum_{j=0}^J \rme^{-\lambda_j t} \phi_j(x) \phi_j(y). }
\end{equation}    
Note that for any $t \geq 0$, $x, y\in \M$,
$S_{J,t}(x,y) \in \mathrm{T}_x \M$.  Under regularity conditions on $\M$ it can be
shown that for any $x,y \in \M$ and $t \geq 0$,
$\lim_{J \to +\infty} S_{J,t} = \nabla_x \log p_{t|0}(x,y)$ \cite[see][Lemma
1]{jones2008Manifold}. In the case of the $d$-dimensional torus or sphere
the eigenvalues and eigenvectors are known, \cite[see][Section
2]{saloff1994precise} and we can apply this method to approximate $p_{t|0}$
for any $t > 0$. We refer to \cref{sec:eigenf-eigenv-lapl} for more details
about eigenvalues and eigenfunctions of the Laplace-Beltrami operator in the
special case of the $d$-dimensional torus and sphere.  \valentin{experiment:
  quality o the approximation as a function of $J$ and $t$. On the same
  graph put the Varadhan approx}

When the eigenvalues and eigenvectors are not tractable, we
can still derive an approximation of the heat kernel for small times $t$. Using
Varadhan's asymptotics---see \citet[Theorem 3.8]{bismut1984large} or
\citet[Theorem 2.1]{chen2021logarithmic}---for any $x, y \in \M$ with
$y \notin \mathrm{Cut}(x)$ (where $\mathrm{Cut}(x)$ is the cut-locus of $x$
in $\M$) we have that \cite[see][Chapter 10]{lee2018introduction} 
\begin{equation}
  \label{eq:varadhan}
  \textstyle{\lim_{t \to 0} t \nabla_y \log p_{t|0}(x,y) = - \exp^{-1}_x(y) . }
\end{equation}
Note that since the cut-locus has null measure under the uniform distribution
$\piinv$ \citep[Theorem 10.34]{lee2006riemannian}, the previous relation is
valid almost everywhere.  We will see in \cref{sec:riem-score-appr} that an
approximation for any $x, y \in \M$ of $t \nabla_y \log p_t(x,y)$ for small
values of $t \geq 0$ is enough to define a score approximation.


\subsection{A manifold time-reversal formula}
\label{sec:time-revers-form}

After having defined the forward noising process targeting a reference
distribution, a second key ingredient of SGMs is to derive a time-reversal
formula. Namely, if $(\bfX_t)_{t \in \ccint{0,T}}$ is a diffusion process then
$(\bfX_{T-t})_{t \in \ccint{0,T}}$ is also a diffusion process w.r.t.\ the
backward filtration whose coefficients can be computed, see
\cref{sec:time-reversal}.  Our next result is the Riemannian
counterpart to the Euclidean time-reversal formula, see \citet[Theorem
4.9]{cattiaux2021time} and \citet{haussmann1986time} for instance, which states
under mild regularity and integrability conditions if the $\rset^d$-valued
process $(\bfX_t)_{t \in \ccint{0,T}}$ is a (weak) solution to the SDE
\begin{equation}
  \rmd \bfX_t = b(\bfX_t) \rmd t +  \rmd  \bfB_t  , 
\end{equation}
then $(\bfY_t)_{t \in \ccint{0,T}} = (\bfX_{T-t})_{t \in \ccint{0,T}}$ is a
(weak) solution to the SDE
\begin{equation}
  \rmd \bfY_t = \{-b(\bfY_t) + \nabla \log p_{T-t}(\bfY_t) \}\rmd t + \rmd \bfB_t  , 
\end{equation}
In the case where $(\bfX_t)_{t \in \ccint{0,T}}$ is an Ornstein-Ulhenbeck
process, then we recover \cref{eq:backward_SDE}. 

\begin{theorem}[Reverse diffusion]
  \label{thm:time_reversal_manifold}
  Let $T \geq 0$ and $(\bfB_t^\M)_{t \geq 0}$ be a Brownian motion on $\M$ such
  that $\bfB_0^\M$ has distribution $\piinv$.  Let
  $(\bfX_t)_{t \in \ccint{0,T}}$ associated with the SDE
  $\rmd \bfX_t = b(\bfX_t) \rmd t + \rmd \bfB_t^\M$.  Let
  $(\bfY_t)_{t \in \ccint{0,T}} = (\bfX_{T-t})_{t \in \ccint{0,T}}$ and assume
  that $\KLLigne{\Pbb}{\Qbb} < +\infty$, where
  $\Qbb \in \Pens(\rmc(\ccint{0,T}, \M))$ is the distribution of
  $(\bfB_t^\M)_{t \in \ccint{0,T}}$. In addition, assume that for any
  $t \in \ccint{0,T}$, $\Pbb_t$ admits a smooth positive density $p_t$ w.r.t.\
  $\piinv$. Then, we have that $(\bfY_t)_{t \in \ccint{0,T}}$ is associated with
  the SDE
  \begin{equation}
    \label{eq:time_reversal_manifold}
   \rmd \bfY_t = \{-b(\bfY_t) + \nabla \log p_{T-t}(\bfY_t)\} \rmd t + \rmd \bfB_t^\M. 
  \end{equation}
\end{theorem}

\begin{proof}
  The proof is a smooth extension of \citet[Theorem 4.9]{cattiaux2021time} to the
  Riemannian manifold case. We postpone the detailed proof to
  \cref{sec:time-reversal}.
\end{proof}

Note the formula obtained for the drift of the time reversed process are the
same in the Euclidean and the Riemannian settings upon replacing the Euclidean
gradient operator, Laplacian and scalar product by the Riemannian ones.  As a
corollary of \cref{thm:time_reversal_manifold}, we get the following result.

% Recall that in the case where the manifold is isometrically embedded in $\rset^p$
% for some $p \geq d$ then $(\bfB_t^\M)_{t \in \ccint{0,T}}$ satisfies \eqref{eq:brownian_motion_extrinsic}.
% Therefore, if we let $(\bfY_t)_{t \in \ccint{0,T}}= (\bfB_{T-t}^\M)_{t \in \ccint{0,T}} $ we have that for any $j \in \{1, \dots, p\}$
% \begin{equation}
% ....
% \end{equation}
%\valentin{I dont even see now how we can get the extrinsic result!}

\begin{corollary}
  Under the conditions of \cref{thm:time_reversal_manifold}, denote
  $(\Pker_t)_{t \in \ccint{0,T}}$ and
  $(\Qker_{s,t})_{s, t \in \ccint{0,T}, t\geq s}$ the semi-group associated with
  $\Pbb$, respectively $R(\Pbb)$. Then:
  \begin{enumerate}[label= (\alph*),  wide, labelwidth=!, labelindent=0pt]
  \item $(\Pker_t)_{t \in \ccint{0,T}}$ is associated with the generator
          $\generator: \ \rmc^\infty(\M) \to \rmc^\infty(\M)$ given for any
      $f \in \rmc^\infty(\M)$ by
      $\generator(f) = \langle b, \nabla f \rangle_{\M} + (1/2) \Delta_\M f$.
  \item $(\Qker_{t|s})_{s,t \in \ccint{0,T}, t\geq s}$ is associated with the family of  generators $(\generator_u)_{u \in \ccint{0,T}}$ such that for any $u \in \ccint{0,T}$, 
          $\generator_u: \ \rmc^\infty(\M) \to \rmc^\infty(\M)$ is given for any
      $f \in \rmc^\infty(\M)$ by
      $\generator_u(f) = \langle -b + \nabla \log p_{T-u}, \nabla f \rangle_{\M} + (1/2) \Delta_\M f$.
  \end{enumerate}
\end{corollary}

In particular, we can approximate the time-reversed process using a GRW using
\cref{thm:grw_diffusion}.

\subsection{Score approximation on Riemannian manifolds}
\label{sec:riem-score-appr}

The last ingredient in order to define the (compact) Riemannian manifold
extension of SGM is an approximation of the logarithmic gradient appearing in
\cref{eq:time_reversal_manifold}.


\paragraph{Score-matching and loss functions}
We aim to approximate $\nabla \log p_t(x)$ for every $t \in \ocint{0,T}$ and
$x \in \M$. To do so, we first remark that for any $s,t \in \ocint{0,T}$ with $t > s$ and
$x_t \in \M$, $p_t(x_t) = \int_{\M} p_{t|s}(x_t|x_s) \rmd \Pbb_s(x_s)$.  Therefore,
we obtain that for any $s, t \in \ccint{0,T}$ with $t > s$ and $x_t \in \M$
  \begin{equation}
  \textstyle{
    \nabla \log p_t(x_t) = \int_{\M} \nabla_x \log p_{t|s}(x_t|x_s) \Pker_{s|t}(x_t, \rmd x_s)  .
    }    
  \end{equation}
  Hence, for any $s, t \in \ccint{0,T}$ with $t > s$ we have that
  \begin{equation}
    \textstyle{
      \nabla \log p_t = \argmin \ensembleLigne{\ell_{t|s}(s_t)}{s_t \in \rmL^2(\Pbb_t)}  , \quad \ell_{t|s}(s_t) = \int_{\M \times \M} \normLigne{\nabla_x \log p_{t|s}(x_t|x_s) - s_t(x_t)}^2 \rmd \Pbb_{s,t}(x_s,x_t)   .
      }
    \end{equation}
    The loss function $\ell_{t|s}$ is called the Denoising Score Matching (DSM)
    loss. It can also be written in an \emph{implicit} fashion.
    \begin{proposition}
      \label{prop:implicit_der}
      Let $t \in \ocint{0,T}$. If $s_t \in \rmc^\infty(\M)$ then we have that  $\ell_{t|s}(s_t) = 2 \ellim_t(s_t) + \int_{\M \times \M} \normLigne{\nabla \log p_{t|s}(x_t|x_s)}^2 \rmd \Pbb_{s,t}(x_s,x_t)$, where
      \begin{equation}
        \textstyle{
          \ellim_t(s_t) = \int_\M \{ \tfrac{1}{2}\normLigne{s_t(x_t)}^2 + \dive(s_t)(x_t) \}  \rmd \Pbb_t(x_t)  . 
          }
      \end{equation}

    \end{proposition}

    \begin{proof}
      The proof is postponed to \Cref{sec:implicit-losses}.
    \end{proof}

    For any $t \in \ocint{0,T}$ the minimizers of the loss $\ellim_t$ on $\XM$
    are the same as the ones for $\ell_{t|s}$. The loss $\ellim_t$ is called the
    \emph{implicit} score matching (ISM) loss (or sliced score matching (SSM)
    loss if the divergence is approximated using the Hutchinson's trace
    estimator \cite{hutchinson1989stochastic}).  Depending on the assumptions on
    the specific manifold at hand it may be more convenient to use $\ell_{t|s}$
    or $\ellim_t$.  Assume that we have access to
    $\ensembleLigne{\nabla \log p_{t|s}}{s, t \in \ccint{0,T}, \ t > s}$ or an
    approximation of this family, then we can use $\ell_{t|s}$, the
    \emph{explicit} score function to learn
    $\ensembleLigne{s_t}{t \in \ccint{0,t}}$. Using the results of
    \cref{sec:brown-moti-comp}, we highlight to methods to approximate
    $\ell_{t|s}$:
    \begin{enumerate}[label= (\alph*),  wide, labelwidth=!, labelindent=0pt]
    \item If we have access to an approximation of
      $\ensembleLigne{p_{t|0}}{t \in \ocint{0,T}, \ t}$ then $\ell_{t|0}$ can be
      used. Note that this loss is similar to the one used in the Euclidean
      setting, see
      \citep{song2020score,song2020improved,song2020denoising,ho2020denoising}
      for instance. In the case, where the eigenvalues and the eigenfunctions of
      the Laplace-Beltrami operator are known then such an approximation is
      available, see \cref{sec:brown-moti-comp}. However, the quality of the
      approximation deteriorates when $t$ is close to
      $0$. % In particular, in the case of
      % the sphere or the torus, we use this loss function as our baseline, see
      % \cref{sec:experiments}. \valentin{TO MODIFY}
    \item If we do not have access to the eigenvalues and eigenfunctions of the
      Laplace-Beltrami operators then we can still derive an approximation of
      the $\nabla \log p_{t|s}$ for all $s \in \ccint{0,t}$ if $\abs{t-s}$ is
      small enough, using Varadahn type approximations \eqref{eq:varadhan} and
      the inverse of $\exp$\footnote{If $\exp^{-1}$ is not available then it can
        be estimated using approximated logarithmic mappings
        \citep{goto2021approximated,schiela2020sqp} or inverse retractions
        \citep{zhu2020riemannian,sato2019riemannian}.}. In this case we use the
      loss functions $\ell_{t|s}$ for $\absLigne{t-s}$ small enough.
    \end{enumerate}
    We highlight that these two methods can be used in conjunction. For
    instance, one can rely on the truncation techniques to estimate $\ell_{t|0}$
    for large $t$ and the Varadhan asymptotics for small $t$.
    
    Last but not least, the \emph{implicit} score loss $\{\ellim_t\}_{t=0}^T$ is
    used in cases where we do not have access to the approximations of $p_{t|s}$
    for $s,t \in \ccint{0,T}$ with $t > s$. The only requirement to learn the
    implicit score is to be able to (approximately) sample from the forward
    dynamics, i.e. the Brownian motion on the Riemannian manifold. In
    particular, no approximation of the logarithmic derivative of the heat
    kernel is needed. One downside of using such an approach is that it relies
    on the computation of the divergence of the score $s_t$. The exact
    computation of the divergence is too costly in high dimension as it requires
    $d$ Jacobian-vector calls and estimators need to be used
    \cite{hutchinson1989stochastic}. Note that the loss function used in
    \citep{rozen2021moser} also involves computing a divergence. We summarize our
    different loss functions in \cref{tab:sm_losses}.

\begin{table}[h]
\centering
\small
\renewcommand*{\arraystretch}{1.4}
\begin{tabular}{ccl}
Method & Loss function  & Requirements \\
\midrule
$\ell_{t|0}$ (DSM)   &  $\frac{1}{2} \E \left[ \| s(\bfX_t) - \nabla \log p_{t|0}(\bfX_t|\bfX_0) \|^2 \right]$ &  \vtop{\hbox{\strut $\triangleright$ Sampling of $(\bfX_t, \bfX_0)$}\hbox{$\triangleright$ Approximation of $\nabla \log p_{t|0}$}}  \\ %\hline
$\ell_{t|s}$ (DSM)   &  $\frac{1}{2} \E \left[ \| s(\bfX_t) - \nabla \log p_{t|0}(\bfX_t|\bfX_s) \|^2 \right]$ & \vtop{\hbox{\strut $\triangleright$ Sampling of $(\bfX_t, \bfX_s)$ for $\abs{t-s}$ small}\hbox{$\triangleright$ Approximation of $\nabla \log p_{t|s}$ for $\abs{t-s}$ small}} \\ %\hline
$\ellim_t$ (ISM)  &  $\E \left[\frac{1}{2} \| s(\bfX_t) \|^2 + \dive( s)(\bfX_t)  \right]$  & \vtop{\hbox{\strut $\triangleright$ Sampling of $\bfX_t$}\hbox{$\triangleright$ Approximation of $\dive(\bm{s}_\theta)$}}
\end{tabular}
\caption{\small Riemannian score matching losses.}
\label{tab:sm_losses}
\end{table}

\paragraph{Parametric family of vector fields}
% We need to define a parametric family of functions in order to
We approximate $\{\nabla \log p_t\}_{t=0}^T$ by a
family of function $\{\bm{s}_\theta\}_{\theta \in \Theta}$ where $\Theta$ is a
set of parameters and for any $\theta \in \Theta$,
$\bm{s}_\theta: \ \ccint{0,T} \to \XM$. In this work, we consider several
parameterisations of vector fields:
%
\begin{itemize}
\item \textbf{Projected vector field}. We define
  $\bm{s}_\theta(t, x) = \text{proj}_{T_{x}\M}(\tilde{\bm{s}}_\theta(t, x)) = P(x)
  \tilde{\bm{s}}_\theta(t, x) $ for any $t \in \ccint{0,T}$ and $x \in \M$, with
  $\tilde{\bm{s}}_\theta: \ \rset^p \times \ccint{0,T} \to \rset^p$ an ambient vector
  field and $P(x)$ the orthogonal projection over $\mathrm{T}_x\M$ at $x \in M$.
  According to \citet[Lemma 2]{rozen2021moser}, then
  $\dive(s_\theta)(x,t) = \dive_E(s_\theta)(x,t)$ for any $x \in \M$, where
  $\dive_E$ denotes the standard Euclidean divergence.
    % \mjh{We use the same trick in the vec field GP paper} \emile{We also do that in practice in \citep{mathieu2020riemannian}}
    

\item \textbf{Divergence-free vector fields}: For any compact \valentin{I think
    this is needed? Maybe not} Lie group, any basis of the Lie algebra $\mathfrak{g}$
  yields a global frame. Indeed, let $v \in \mathfrak{g}$ and define the flow
  $\Phi: \ \rset \times \M \to \M$ given for any $t \in \rset$ and $x \in M$ by
  $\Phi_t^v(x) = x \exp(t v)$. Then defining
  $\{E_i\}_{i=1}^d = \{\partial_t \Phi_0^{v_i}\}_{i=1}^d$, where
  $\{v_i\}_{i=1}^d$ is a basis of $\mathfrak{g}$, we get that $\{E_i\}_{i=1}^d$
  is a left-invariant global frame. As a result, we have that for any
  $i \in \{1, \dots, d\}$, $\dive(E_i)=0$ (for the classical left invariant
  metric). This result simplifies the computation of $\dive(\bm{s}_\theta)$ where
  $\bm{s}_\theta(t,s) = \sum_{i=1}^d \bm{s}^i_\theta(t,x) E_i(x)$ for any
  $t \in \ccint{0,T}$ and $x \in \M$ \cite[see][]{falorsi2020neural}.
%   We define
%   $s_\theta(x, t) = \sum_{i=1}^d s^i_\theta(x, t) f^i(x)$ for any
%   $t \in \ccint{0,T}$ and $x \in \M$, where for any $i \in \{1, \dots, d\}$,
%   $f_i(x) = \partial_t \exp( \cdot \xi_i)(0) \cdot x$ (with some frame
%   $\{\xi_i\}_{i=1}^d$). For any homogeneous space, this family of vector fields
%   generates the tangent bundle and is divergence-free \valentin{ref}.
%     % \mjh{Is the divergence free nature of the field going to impose some extra constraint on the score function, and the resulting process?}
%     % The $f^i$ are divergence free but the linear combination which yields $s_\theta$ has no constraint.
%   Then for any $t \in \ccint{0,T}$ and $x \in \M$, we have
%   $\dive(s_\theta)(x, t) = \sum_{i=1}^d \dive(s^i_\theta(\cdot, t) f^i)(x)  = \sum_{i=1}^d \langle \nabla
%     s^i_\theta(x, t), f^i(x)\rangle$. \valentin{this can be estimated stochastically. How?}
% %    For more information see notes at \url{https://www.overleaf.com/read/thvfprqwmkjq}.
%     % Not sure whether this can be useful here but this term does appear in standard (non-denoising) score matching \citep{hyvarinenEstimation,song2019Sliced}. }
\item \textbf{Coordinates vector fields}. We define
  $\bm{s}_\theta(t, x) = \sum_{i=1}^d \bm{s}^i_\theta(t,x) E_i(x)$ for any
  $t \in \ccint{0,T}$ and $x \in \M$, with
  $\{E_i\}_{i=1}^d = \{\partial_i \varphi(x)\}_{i=1}^d$ the vector fields
  induced by a choice of local coordinates, where $\varphi$ is a local
  parameterization $\varphi: \ \msu \to \M$ and $z \in \msu \subset
  \rset^d$. Then the divergence can be computed in these local coordinates
  $\dive(\bm{s}_\theta)(t, \varphi(z)) =\absLigne{\det G}^{-1/2} \sum_{i=1}^d
  \partial_i \{ \absLigne{\det G}^{1/2} \bm{s}^i_\theta(t,
  \varphi(\cdot))\}(z)$. In the case of the sphere, one recovers the standard
  divergence in spherical coordinates using this formula.
  % If the manifold is not
  % parallelisable, there does not exist a global frame, which implies that
  % $\{f^i(x)\}$ is not a basis for any $x \in \M$. Think of the Hairy-ball
  % theorem for the (n-)sphere.
  % \mjh{but we don't actually need a basis. We just need a smooth set of basis
  % vector fields that span the tangent space. The fields can be redundant
  % e.g. 3 axis fields on the sphere. We then just mix these smooth fields with
  % smooth scalar coeffs from an nn and mix them to get a smooth field}
  % \emile{Agree that we don't need a basis, we only need a generator of the
  % tangent bundle.}
\end{itemize}
\emile{We do not discuss NN architectural choices for $\{s_\theta^i\}_i$ but can do for the next iteration.}
%
Combining this parameterization with the score-matching losses, the
time-reversal formula \cref{sec:time-revers-form} and the sampling of forward
and backward processes \cref{sec:brown-moti-comp}, we now define our Riemannian
Score-based Generative Modeling algorithm, in \cref{alg:rsgm}.


  \begin{algorithm}[!t]
   \caption{\small Computation of the loss}
   \label{alg:rsgm}
   \begin{algorithmic}[1]
     \small
     \Require $\vareps, T, K, \pizero, \mathrm{loss}, \mathrm{thres}, s$
     \State $\bfX_0 \sim \pizero$
     \State $t \sim U(\ccint{\vareps, T})$ \Comment Uniform sampling between $\vareps$ and $T$
     \State $\bfX_t \sim \Pker_{t|0}(\bfX_0, \cdot)$ \Comment Approximate sampling using \cref{alg:grw} 
     \If{$\mathrm{loss = denoising}$} \Comment Denoising loss function
     \If{$t < \mathrm{thres}$}
     \State $\mathrm{score} = -(1/t) \exp^{-1}_{\bfX_t}(\bfX_0)$ \Comment Varadhan asymptotics
     \Else
     \State $\mathrm{score} = \sum_{j=0}^J \rme^{-\lambda_j t} \nabla \phi_j(x) \phi_j(y) / \sum_{j=0}^J \rme^{-\lambda_j t} \phi_j(x) \phi_j(y).$ \Comment Series truncation
     \EndIf
     \State $\ell(s) = \norm{s(\bfX_t) - \mathrm{score}}^2$
     \Else \Comment Implicit loss function
     \State $\ell(s) = (1/2) \norm{s(\bfX_t)}^2 + \dive(s)(\bfX_t)$
     \EndIf
      \State {\bfseries return} $\ell(s)$
    \end{algorithmic}
    \end{algorithm}


\subsection{Likelihood computation}
\label{sec:likel-comp}

Similarly to \cite{song2020score}, once the score is learned we can use it
in conjunction with an Ordinary Differential Equation (ODE) solver to compute
the likelihood of the model. Let $\{\Phi_t\}_{t=0}^T$ be a family of vector
fields. We define $(\bfX_t)_{t \in \ccint{0,T}}$ such that $\bfX_0$ has
distribution $p_0$ (the data distribution) and satisfying
$\rmd \bfX_t = \Phi_t(\bfX_t) \rmd t$. Assuming that  $p_0$ admits a density
w.r.t.\ $\piinv$ then for any $t \in \ccint{0,T}$, the distribution of $\bfX_t$
admits a density w.r.t.\ $\piinv$ and we denote $p_t$ this density.  We recall that
$\partial_t \log p_t(\bfX_t) = \dive(\Phi_t)(\bfX_t)$, see \citet[Proposition 
2]{mathieu2020riemannian} for instance.

Recall that we consider a Brownian motion on the manifold as a forward process
$(\bfB_t^\M)_{t \in \ccint{0,T}}$ with $\{p_t\}_{t=0}^T$ the associated family
of densities. Thus we have that for any $t \in \ccint{0,T}$ and $x \in \M$
\begin{equation}
  \partial_t p_t(x) = \tfrac{1}{2} \Delta p_t(x) = \dive\left(\tfrac{1}{2} p_t \nabla \log p_t \right)(x)  . 
\end{equation}
Hence, we can define $(\bfX_t)_{t \in \ccint{0,T}}$ satisfying
$\rmd \bfX_t = \tfrac{1}{2} \nabla \log p_t(\bfX_t) \rmd t$ such that $\bfX_0$ has
distribution $p_0$.
Defining
$(\bfhX_t)_{t \in \ccint{0,T}} = (\bfX_{T-t})_{t \in \ccint{0,T}}$, it follows
that $\bfhX_0$ has distribution $\mathcal{L}(\bfX_T)$ and satisfies
\begin{equation}
  \label{eq:backward_flow}
 \rmd \bfhX_t =-\tfrac{1}{2} \nabla \log p_{T-t}(\bfhX_t) \rmd t  . 
\end{equation}
Finally, we introduce $(\bfY_t)_{t \in \ccint{0,T}}$ satisfying
\eqref{eq:backward_flow} but such that $\bfY_0 \sim \piinv$.  Note
that if $T \geq 0$ is large then the two processes
$(\bfY_t)_{t \in \ccint{0,T}}$ and $(\bfhX_t)_{t \in \ccint{0,T}}$ are close
since $\mathcal{L}(\bfX_T)$ is close to $\piinv$.  Therefore, using the score
network and a manifold ODE solver \citep[as in][]{mathieu2020riemannian}, we
are able to approximately solve the following ODE
\begin{equation}
  \partial_t \log q_t(\bfY_t) = -\tfrac{1}{2}\dive(\bm{s}_\theta(t,\cdot))(\bfY_t)  ,
\end{equation}
with $q_t$ the density of $\bfY_t$ w.r.t.\ $\piinv$ and $\log q_0(\bfY_0) =
0$. The likelihood approximation of the model is then given by $\log q_T(\bfY_T)$. In
\cref{sec:diff-betw-ode}, we highlight that this likelihood computation is
slightly different from the one obtained using the SDE.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
