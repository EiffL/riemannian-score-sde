\section{Introduction}
\label{sec:introduction}

Score-based Generative Modeling (SGM) is a recently developed approach to
generative modeling exhibiting state-of-the-art performances on various tasks
including image and audio synthesis
D\citep{song2019generative,song2020score,ho2020denoising,nichol2021improved,nichol2021beatgans}. These
models proceed as follows. We add noise to the data progressively using a
diffusion process targeting a reference Gaussian distribution. The corresponding
time-reversal process is also a diffusion whose drift depends on the logarithmic
gradients of the perturbed data distributions, i.e. the scores. The generative
model is obtained by approximating this time-reversal denoising diffusion by
initializing it at the reference Gaussian distribution and using neural networks
estimates of the scores obtained using score matching
\cite{hyvarinen2005estimation,vincent2011connection}. It can be shown rigorously
that the obtained final samples are approximately distributed according to the
data distribution \citep{debortoli2021neurips}.

Until now, SGM has been applied to Euclidean data, i.e. data with flat
geometry. However, in a large number of scientific domains, the underlying
assumption is that the distributions of interest are supported on a Riemannian
manifold. These include, amongst others, protein modeling
\citep{boomsma2008generative,hamelryck2006sampling,mardia2008multivariate,shapovalov2011smoothed,mardia2007protein},
cell development \citep{klimovskaia2020poincare}, image recognition
\citep{lui2012advances}, geological sciences
\citep{karpatne2018machine,peel2001fitting}, graph-structured and hierarchical
data \citep{roy2007learning,steyvers2005large}, robotics
\citep{feiten2013rigid,senanayake2018directional} and high-energy physics
\citep{brehmer2020flows}. The choice of a Riemannian metric is associated with a
description of the interactions between the points of the dataset and therefore
can be seen as a geometric prior.

In this paper we introduce \emph{Riemannian Score-based Generative Models}
(RSGM), an extension of SGMs to compact Riemannian manifolds. Contrary to
classical SGMs which rely on forward and time-reversed diffusion processes
defined on an Euclidean space, we incorporate the geometry of the data in our
algorithm by defining our diffusion processes directly on the Riemannian
manifold. However, switching from the classical Euclidean setting to the
Riemannian is non-trivial. First, one must be able to define a noising process
on the manifold that converges to an easy-to-sample reference distribution. In
the setting of compact Riemannian manifolds, a natural choice is given by the
Brownian motion. Indeed, due to the compactness, this diffusion is geometrically
ergodic and targets the uniform distribution on the manifold \citep{he2013lower}
from which one can either sample exactly or approximately with high
accuracy. Second, we must identify the corresponding time-reversal process. We
show here that, as in the Euclidean case, this process is also a diffusion whose
infinitesimal generator is given by the generator of the forward process with an
extra term corresponding to the scores of the marginal distributions of the
Brownian diffusion initialized at the data distribution. Third, while score
matching ideas \citep{hyvarinen2005estimation,vincent2011connection} can be
easily used to estimate the score in the Euclidean case when the forward
dynamics is given by a Ornstein--Ulhenbeck or a Brownian motion, adapting these
ideas to the Riemmanian framework is complicated by the fact that the heat
kernel, i.e. the transition kernel of the Brownian motion, is typically only
available as an infinite sum through the Sturm-Liouville
decomposition. Similarly, diffusions on manifold cannot be sampled
exactly. Hence, we use geodesic random walks which converge to the diffusion of
interest in the limit of small stepsizes \citep{jorgensen1975central}.

We further consider the following extensions of RSGMs. By using tools from
neural ODEs on manifolds
\citep{mathieu2020riemannian,falorsi2020neural,lou2020neural}, we show how we
can compute the likelihood of our model, generalizing the approach proposed in
the Euclidean case in
\citep{song2020score,durkan2021maximum,huang2021variational}. Finally, RGSMs
like standard SGMs are computationally expensive at generation time as they
require to run a discretized diffusion over many time steps. For speeding up
generation, it has been proposed in the Euclidean setting to solve instead a
Schr\"odinger Bridge (SB) problem
\citep{debortoli2021neurips,chen2021likelihood}, i.e. a dynamical version of
an entropy-regularized Optimal Transport (OT) problem between the data and the
easy-to-sample reference distribution. In particular, we generalize the
Diffusion Schr\"odinger Bridge (DSB) algorithm introduced in
\citep{debortoli2021neurips} to solve the SB problem on compact Riemmanian
manifolds.

% We also investigate the connection between SGMs and entropy-regularized Optimal Transport
% (OT) \citep{debortoli2021neurips,vargas2021solving,chen2021likelihood}. More
% precisely, we define the Schr\"odinger Bridge (SB) problem on Riemannian
% manifolds, which corresponds to a dynamical version of regularized OT. We solve
% this problem with a procedure akin to Diffusion Schr\"odinger Bridge (DSB)
% introduced in \citep{debortoli2021neurips}.  DSB allows to speed up
% significantly the sampling process in generative modeling and also permits to
% define diffusions interpolating between arbitrary distributions. We show that
% our Riemannian extension of DSB enjoys the same benefits.

We validate our methodology by modelling a number of natural disaster occurrence datasets collected by \cite{mathieu2020riemannian}. We compare to three previous baselines, a mixture of Kent distributions \cite{peel2001fitting}, Riemannian Continuous Normalising Flows \cite{mathieu2020riemannian}, and Moser Flows \cite{rozen2021moser}. We also compare to using a standard standard Euclidean SGM by projecting the manifold onto Euclidean space and performing the flow there (e.g. projecting the sphere via the stereographic projection onto the 2D plane). We find in all cases that RSGMs outperform all baselines.

% We validate our methodology \valentin{TO FILL}. (experiments)
% Code is available at ...

The rest of the paper is organized as follows. We introduce the notation needed
in the rest of the paper in \cref{sec:notation}. We recall the basics of
standard Euclidean SGMs in \Cref{sec:eucl-sgm-riem}. In
\Cref{sec:score-appr-manif}, we present RGSMs, our extension of SGMs to compact
Riemannian manifolds. We discuss related works in
\Cref{sec:related-works} and assess the efficiency of our method in
\Cref{sec:experiments}. % Finally, we present an extension of our work to
% Schr\"odinger bridges in \Cref{sec:extension} and
Finally we summarize our contributions in \Cref{sec:conclusion}.

\section{Notation}
\label{sec:notation}

We consider a compact connected Riemannian manifold
$(\M, \langle \cdot, \cdot \rangle_\M)$. We denote by $\XM$ the set of vector
fields on $\M$ and $\XMdeux$ the section
$\Gamma(\M, \sqcup_{x \in \M} \mathcal{L}(\mathrm{T}_x \M))$, where $\mathcal{L}(\mathrm{T}_x \M)$ is the space of linear mappings on
$\mathrm{T}_x \M$. Let $(\bfM_t)_{t \in \ccint{0,T}}$ be a real-valued process
and $(\bfX_t)_{t \in \ccint{0,T}}$ be a $\M$-valued process with distribution
$\Pbb \in \Pens(\rmc(\ccint{0,T}, \M))$.  $(\bfM_t)_{t \in \ccint{0,T}}$ is a
$\Pbb$-martingale if $(\bfM_t)_{t \in \ccint{0,T}}$ is a martingale w.r.t the
filtration $(\mcf_t)_{t \in \ccint{0,T}}$ where for any $t \in \ccint{0,T}$,
$\mcf_t = \sigma(\ensembleLigne{\bfX_s}{s \in \ccint{0,t}})$. In addition, for
any $\Pbb \in \Pens(\rmc(\ccint{0,T}, \M))$, we define $R(\Pbb)$ such that for
any $\msa \in \mcb{\rmc(\ccint{0,T}, \msx)}$ we have
$R(\Pbb)(\msa) = \Pbb(R(\msa))$, where
$R(\msa) = \ensembleLigne{t \mapsto \omega_{T-t}}{\omega \in \msa}$. In other
words, $R(\Pbb)$ is the path measure associated with the reverse process $\Pbb$.
When there is no ambiguity, we use the same notation for distributions and their
densities.

Let $T > 0$ or $T=+\infty$, $b: \ \ccint{0,T} \to \XM$, $\Sigma: \ \ccint{0,T} \to \XMdeux$
such that for any $t \in \ccint{0,T}$ and $x \in \M$, $\Sigma(t,x)$ is
symmetric, non-negative and denote $\sigma(t,x) = \Sigma^{1/2}(t,x)$. Let
$(\bfX_t)_{t \in \ccint{0,T}}$ a continuous process with distribution
$\Pbb \in \Pens(\rmc(\ccint{0,T}, \M))$ such that for any $f \in \rmc^2(\M)$ we
have that $(\bfM_t^{\bfX, f})_{t \in \ccint{0,T}}$ is a $\Pbb$-martingale where
for any $t \in \ccint{0,T}$
  \begin{equation}
    \textstyle{ \bfM_t^{\bfX, f} = f(\bfX_t) - \int_0^t \{ \langle b(s, \bfX_s), \nabla f(\bfX_s) \rangle_\M + (1/2) \langle \Sigma(\bfX_s),  \nabla^2 f(\bfX_s) \rangle_\M \} \rmd s  . }
  \end{equation}
  Then, we say that $(\bfX_t)_{t \in \ccint{0,T}}$ is \emph{associated with} the
  SDE $\rmd \bfX_t = b(t, \bfX_t) \rmd t + \sigma(t, \bfX_t) \rmd \bfB_t^\M$
  with infinitesimal generator
  $\generator: \ \ccint{0,T} \times \rmc^2(\M) \to \rmc(\M)$ given for any
  $t \in \ccint{0,T}$ by
  $\generator_t( f) = \langle b, \nabla f \rangle_\M + (1/2) \langle \Sigma,
  \nabla^2 f \rangle_\M$. Note that if $\Sigma = \Id$ then
  $\langle \Sigma, \nabla^2 f \rangle_\M = \Delta f$, where $\Delta$ is the
  Laplace-Beltrami operator.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
